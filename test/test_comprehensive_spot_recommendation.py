# -*- coding: utf-8 -*-
"""
æ™¯ç‚¹æ¨èç®—æ³•å…¨é¢æ€§èƒ½æµ‹è¯•æ¡†æ¶
åŒ…å«å¤šç»´åº¦ã€å¤§è§„æ¨¡ã€å‹åŠ›æµ‹è¯•ç­‰å„ç§æµ‹è¯•åœºæ™¯
"""

import time
import random
import threading
import multiprocessing
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from memory_profiler import profile
import psutil
import gc
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from module.user_class import UserManager, userManager
from module.Spot_class import spotManager
from module.Model.Model import User, Spot
import module.printLog as log

class ComprehensiveSpotRecommendationTest:
    """å…¨é¢çš„æ™¯ç‚¹æ¨èæµ‹è¯•æ¡†æ¶"""
    
    def __init__(self):
        self.user_manager = userManager
        self.spot_manager = spotManager
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            'basic_test_sizes': [5, 10, 20, 50, 100, 200, 500],
            'stress_test_sizes': [1000, 2000, 5000, 10000],
            'concurrent_thread_counts': [1, 5, 10, 20, 50],
            'memory_test_iterations': 1000,
            'stability_test_duration': 300,  # 5åˆ†é’Ÿç¨³å®šæ€§æµ‹è¯•
            'repeat_count': 5,  # æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°
        }
        
        # ç»“æœå­˜å‚¨
        self.results = {
            'basic_performance': {'traditional': {}, 'optimized': {}},
            'stress_test': {'traditional': {}, 'optimized': {}},
            'concurrent_test': {'traditional': {}, 'optimized': {}},
            'memory_test': {'traditional': {}, 'optimized': {}},
            'stability_test': {'traditional': {}, 'optimized': {}},
            'scalability_test': {'traditional': {}, 'optimized': {}},
            'edge_case_test': {'traditional': {}, 'optimized': {}},
        }
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = ['execution_time', 'memory_usage', 'cpu_usage', 'throughput', 'error_rate']
          # æµ‹è¯•ç”¨æˆ·æ•°æ®
        self.test_users = []
        self.setup_test_data()
    
    def setup_test_data(self):
        """è®¾ç½®æµ‹è¯•æ•°æ® - ä½¿ç”¨ç°æœ‰ç”¨æˆ·æ•°æ®"""
        print("ğŸ”§ åŠ è½½ç°æœ‰ç”¨æˆ·æ•°æ®è¿›è¡Œæµ‹è¯•...")
        
        # ä½¿ç”¨ç°æœ‰çš„ç”¨æˆ·æ•°æ®
        all_users = self.user_manager.users
        total_users = len(all_users)
        
        if total_users == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç”¨æˆ·æ•°æ®ï¼Œè¯·å…ˆåŠ è½½ç”¨æˆ·æ•°æ®")
            return
        
        # æ ¹æ®æµ‹è¯•éœ€è¦é€‰æ‹©ç”¨æˆ·å­é›†
        # å¦‚æœç”¨æˆ·å¤ªå¤šï¼Œéšæœºé€‰æ‹©ä¸€éƒ¨åˆ†è¿›è¡Œæµ‹è¯•
        max_test_users = min(1000, total_users)  # æœ€å¤šä½¿ç”¨1000ä¸ªç”¨æˆ·è¿›è¡Œæµ‹è¯•
        
        if total_users <= max_test_users:
            # å¦‚æœç”¨æˆ·æ•°é‡ä¸å¤šï¼Œä½¿ç”¨æ‰€æœ‰ç”¨æˆ·
            self.test_users = all_users.copy()
        else:
            # éšæœºé€‰æ‹©ç”¨æˆ·è¿›è¡Œæµ‹è¯•ï¼Œä¿è¯æµ‹è¯•çš„éšæœºæ€§
            self.test_users = random.sample(all_users, max_test_users)
        
        # ç­›é€‰æœ‰åå¥½çš„ç”¨æˆ·ï¼ˆlikes_typeä¸ä¸ºç©ºï¼‰
        users_with_preferences = [user for user in self.test_users 
                                if hasattr(user, 'likes_type') and user.likes_type]
        
        if users_with_preferences:
            self.test_users = users_with_preferences
        
        print(f"âœ… åŠ è½½äº† {len(self.test_users)} ä¸ªç°æœ‰ç”¨æˆ·ç”¨äºæµ‹è¯•")
        print(f"ğŸ“Š æ€»ç”¨æˆ·æ•°: {total_users}, æµ‹è¯•ç”¨æˆ·æ•°: {len(self.test_users)}")
        
        # æ˜¾ç¤ºç”¨æˆ·åå¥½ç±»å‹åˆ†å¸ƒ
        self._analyze_user_preferences()
    
    def measure_performance(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°æ€§èƒ½çš„é€šç”¨æ–¹æ³•"""
        process = psutil.Process()
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        start_cpu = process.cpu_percent()
        start_time = time.perf_counter()
        
        try:
            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            success = True
            error_msg = None
        except Exception as e:
            result = None
            success = False
            error_msg = str(e)
        
        # è®°å½•ç»“æŸçŠ¶æ€
        end_time = time.perf_counter()
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        end_cpu = process.cpu_percent()
        
        metrics = {
            'execution_time': (end_time - start_time) * 1000,  # æ¯«ç§’
            'memory_usage': max(0, end_memory - start_memory),  # MB
            'cpu_usage': max(0, end_cpu - start_cpu),  # %
            'success': success,
            'error_msg': error_msg,
            'result_count': len(result) if result else 0        }
        
        return metrics, result
    
    def run_basic_performance_test(self):
        """åŸºç¡€æ€§èƒ½æµ‹è¯• - ä½¿ç”¨ç°æœ‰ç”¨æˆ·æ•°æ®"""
        print("\nğŸš€ å¼€å§‹åŸºç¡€æ€§èƒ½æµ‹è¯•...")
        
        if not self.test_users:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç”¨æˆ·")
            return
        
        for topK in self.test_config['basic_test_sizes']:
            print(f"æµ‹è¯• TopK = {topK}")
            
            traditional_metrics = []
            optimized_metrics = []
            
            # ä»æµ‹è¯•ç”¨æˆ·ä¸­éšæœºé€‰æ‹©è¿›è¡Œæµ‹è¯•
            test_user_sample = random.sample(
                self.test_users, 
                min(self.test_config['repeat_count'], len(self.test_users))
            )
            
            for test_user in test_user_sample:
                try:
                    # æµ‹è¯•ä¼ ç»Ÿç®—æ³•
                    metrics, _ = self.measure_performance(
                        self.user_manager.getRecommendSpotsTraditional, 
                        test_user.id, topK
                    )
                    traditional_metrics.append(metrics)
                    
                    # æµ‹è¯•ä¼˜åŒ–ç®—æ³•
                    metrics, _ = self.measure_performance(
                        self.user_manager.getRecommendSpots, 
                        test_user.id, topK
                    )
                    optimized_metrics.append(metrics)
                    
                except Exception as e:
                    print(f"âš ï¸  ç”¨æˆ· {test_user.id} æµ‹è¯•å¤±è´¥: {str(e)}")
                    continue
            
            if traditional_metrics and optimized_metrics:
                # è®¡ç®—å¹³å‡å€¼
                self.results['basic_performance']['traditional'][topK] = self._calculate_average_metrics(traditional_metrics)
                self.results['basic_performance']['optimized'][topK] = self._calculate_average_metrics(optimized_metrics)
            else:
                print(f"âŒ TopK={topK} æµ‹è¯•å¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆæ•°æ®")        
        print("âœ… åŸºç¡€æ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    def run_stress_test(self):
        """å‹åŠ›æµ‹è¯• - å¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨ç°æœ‰ç”¨æˆ·"""
        print("\nğŸ’ª å¼€å§‹å‹åŠ›æµ‹è¯•...")
        
        if not self.test_users:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç”¨æˆ·")
            return
        
        for topK in self.test_config['stress_test_sizes']:
            print(f"å‹åŠ›æµ‹è¯• TopK = {topK}")
            
            # é€‰æ‹©å¤šä¸ªç”¨æˆ·è¿›è¡Œæµ‹è¯•ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ ·æœ¬
            test_user_count = min(20, len(self.test_users))
            test_users = random.sample(self.test_users, test_user_count)
            
            traditional_metrics = []
            optimized_metrics = []
            
            for i, user in enumerate(test_users):
                print(f"  æµ‹è¯•ç”¨æˆ· {i+1}/{test_user_count} (ID: {user.id})")
                
                try:
                    # æµ‹è¯•ä¼ ç»Ÿç®—æ³•
                    metrics, _ = self.measure_performance(
                        self.user_manager.getRecommendSpotsTraditional, 
                        user.id, topK
                    )
                    traditional_metrics.append(metrics)
                    
                    # æµ‹è¯•ä¼˜åŒ–ç®—æ³•
                    metrics, _ = self.measure_performance(
                        self.user_manager.getRecommendSpots, 
                        user.id, topK
                    )
                    optimized_metrics.append(metrics)
                    
                except Exception as e:
                    print(f"    âš ï¸  ç”¨æˆ· {user.id} æµ‹è¯•å¤±è´¥: {str(e)}")
                    continue
            
            if traditional_metrics and optimized_metrics:
                self.results['stress_test']['traditional'][topK] = self._calculate_average_metrics(traditional_metrics)
                self.results['stress_test']['optimized'][topK] = self._calculate_average_metrics(optimized_metrics)
                
                # æ˜¾ç¤ºå‹åŠ›æµ‹è¯•ç»“æœæ‘˜è¦
                trad_avg_time = self.results['stress_test']['traditional'][topK]['execution_time']
                opt_avg_time = self.results['stress_test']['optimized'][topK]['execution_time']
                improvement = ((trad_avg_time - opt_avg_time) / trad_avg_time) * 100 if trad_avg_time > 0 else 0
                print(f"    ğŸ“Š å¹³å‡æ‰§è¡Œæ—¶é—´ - ä¼ ç»Ÿ: {trad_avg_time:.2f}ms, ä¼˜åŒ–: {opt_avg_time:.2f}ms, æå‡: {improvement:.1f}%")
            else:
                print(f"âŒ TopK={topK} å‹åŠ›æµ‹è¯•å¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        
        print("âœ… å‹åŠ›æµ‹è¯•å®Œæˆ")
    
    def run_concurrent_test(self):
        """å¹¶å‘æµ‹è¯•"""
        print("\nğŸ”„ å¼€å§‹å¹¶å‘æµ‹è¯•...")
        
        def worker_function(algorithm_type, user_id, topK):
            """å·¥ä½œå‡½æ•°"""
            if algorithm_type == 'traditional':
                return self.measure_performance(
                    self.user_manager.getRecommendSpotsTraditional, 
                    user_id, topK
                )
            else:
                return self.measure_performance(
                    self.user_manager.getRecommendSpots, 
                    user_id, topK
                )
        
        for thread_count in self.test_config['concurrent_thread_counts']:
            print(f"å¹¶å‘æµ‹è¯• - {thread_count} çº¿ç¨‹")
            
            topK = 50  # å›ºå®štopKè¿›è¡Œå¹¶å‘æµ‹è¯•
            
            # ä¼ ç»Ÿç®—æ³•å¹¶å‘æµ‹è¯•
            with ThreadPoolExecutor(max_workers=thread_count) as executor:
                start_time = time.perf_counter()
                
                futures = []
                for _ in range(thread_count * 2):  # æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œ2æ¬¡
                    user = random.choice(self.test_users)
                    future = executor.submit(worker_function, 'traditional', user.id, topK)
                    futures.append(future)
                
                traditional_results = [future.result() for future in futures]
                traditional_time = time.perf_counter() - start_time
            
            # ä¼˜åŒ–ç®—æ³•å¹¶å‘æµ‹è¯•
            with ThreadPoolExecutor(max_workers=thread_count) as executor:
                start_time = time.perf_counter()
                
                futures = []
                for _ in range(thread_count * 2):
                    user = random.choice(self.test_users)
                    future = executor.submit(worker_function, 'optimized', user.id, topK)
                    futures.append(future)
                
                optimized_results = [future.result() for future in futures]
                optimized_time = time.perf_counter() - start_time
            
            # è®¡ç®—ååé‡
            traditional_throughput = len(traditional_results) / traditional_time
            optimized_throughput = len(optimized_results) / optimized_time
            
            self.results['concurrent_test']['traditional'][thread_count] = {
                'total_time': traditional_time,
                'throughput': traditional_throughput,
                'metrics': self._calculate_average_metrics([r[0] for r in traditional_results])
            }
            
            self.results['concurrent_test']['optimized'][thread_count] = {
                'total_time': optimized_time,
                'throughput': optimized_throughput,
                'metrics': self._calculate_average_metrics([r[0] for r in optimized_results])
            }
        
        print("âœ… å¹¶å‘æµ‹è¯•å®Œæˆ")
    
    def run_memory_test(self):
        """å†…å­˜æµ‹è¯•"""
        print("\nğŸ§  å¼€å§‹å†…å­˜æµ‹è¯•...")
        
        def memory_intensive_test(algorithm_type, iterations):
            """å†…å­˜å¯†é›†å‹æµ‹è¯•"""
            results = []
            peak_memory = 0
            
            for i in range(iterations):
                user = random.choice(self.test_users)
                topK = random.randint(10, 100)
                
                if algorithm_type == 'traditional':
                    metrics, _ = self.measure_performance(
                        self.user_manager.getRecommendSpotsTraditional, 
                        user.id, topK
                    )
                else:
                    metrics, _ = self.measure_performance(
                        self.user_manager.getRecommendSpots, 
                        user.id, topK
                    )
                
                results.append(metrics)
                peak_memory = max(peak_memory, metrics['memory_usage'])
                
                # æ¯100æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶
                if i % 100 == 0:
                    gc.collect()
            
            return results, peak_memory
        
        # å†…å­˜æµ‹è¯•
        iterations = self.test_config['memory_test_iterations']
        
        traditional_results, traditional_peak = memory_intensive_test('traditional', iterations)
        optimized_results, optimized_peak = memory_intensive_test('optimized', iterations)
        
        self.results['memory_test']['traditional'] = {
            'average_metrics': self._calculate_average_metrics(traditional_results),
            'peak_memory': traditional_peak,
            'total_iterations': iterations
        }
        
        self.results['memory_test']['optimized'] = {
            'average_metrics': self._calculate_average_metrics(optimized_results),
            'peak_memory': optimized_peak,
            'total_iterations': iterations
        }
        
        print("âœ… å†…å­˜æµ‹è¯•å®Œæˆ")
    
    def run_stability_test(self):
        """ç¨³å®šæ€§æµ‹è¯•"""
        print("\nâš–ï¸ å¼€å§‹ç¨³å®šæ€§æµ‹è¯•...")
        
        duration = self.test_config['stability_test_duration']
        
        def stability_worker(algorithm_type, duration):
            """ç¨³å®šæ€§æµ‹è¯•å·¥ä½œå‡½æ•°"""
            start_time = time.time()
            results = []
            error_count = 0
            
            while time.time() - start_time < duration:
                user = random.choice(self.test_users)
                topK = random.randint(10, 100)
                
                try:
                    if algorithm_type == 'traditional':
                        metrics, _ = self.measure_performance(
                            self.user_manager.getRecommendSpotsTraditional, 
                            user.id, topK
                        )
                    else:
                        metrics, _ = self.measure_performance(
                            self.user_manager.getRecommendSpots, 
                            user.id, topK
                        )
                    
                    results.append(metrics)
                    
                    if not metrics['success']:
                        error_count += 1
                        
                except Exception as e:
                    error_count += 1
                
                time.sleep(0.01)  # å°å»¶è¿Ÿé¿å…è¿‡åº¦å ç”¨CPU
            
            return results, error_count
        
        # è¿è¡Œç¨³å®šæ€§æµ‹è¯•
        traditional_results, traditional_errors = stability_worker('traditional', duration)
        optimized_results, optimized_errors = stability_worker('optimized', duration)
        
        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        traditional_error_rate = traditional_errors / len(traditional_results) if traditional_results else 1
        optimized_error_rate = optimized_errors / len(optimized_results) if optimized_results else 1
        
        # è®¡ç®—æ€§èƒ½å˜å¼‚ç³»æ•°
        traditional_times = [r['execution_time'] for r in traditional_results if r['success']]
        optimized_times = [r['execution_time'] for r in optimized_results if r['success']]
        
        traditional_cv = np.std(traditional_times) / np.mean(traditional_times) if traditional_times else float('inf')
        optimized_cv = np.std(optimized_times) / np.mean(optimized_times) if optimized_times else float('inf')
        
        self.results['stability_test']['traditional'] = {
            'total_requests': len(traditional_results),
            'error_count': traditional_errors,
            'error_rate': traditional_error_rate,
            'coefficient_of_variation': traditional_cv,
            'average_metrics': self._calculate_average_metrics(traditional_results)
        }
        
        self.results['stability_test']['optimized'] = {
            'total_requests': len(optimized_results),
            'error_count': optimized_errors,
            'error_rate': optimized_error_rate,
            'coefficient_of_variation': optimized_cv,
            'average_metrics': self._calculate_average_metrics(optimized_results)
        }
        
        print("âœ… ç¨³å®šæ€§æµ‹è¯•å®Œæˆ")
    
    def run_scalability_test(self):
        """å¯æ‰©å±•æ€§æµ‹è¯•"""
        print("\nğŸ“ˆ å¼€å§‹å¯æ‰©å±•æ€§æµ‹è¯•...")
        
        # æµ‹è¯•ä¸åŒç”¨æˆ·æ•°é‡ä¸‹çš„æ€§èƒ½
        user_counts = [1, 5, 10, 25, 50, 100]
        
        for user_count in user_counts:
            print(f"å¯æ‰©å±•æ€§æµ‹è¯• - {user_count} ç”¨æˆ·")
            
            test_users = random.sample(self.test_users, min(user_count, len(self.test_users)))
            topK = 50
            
            # æµ‹è¯•ä¼ ç»Ÿç®—æ³•
            start_time = time.perf_counter()
            traditional_results = []
            for user in test_users:
                metrics, _ = self.measure_performance(
                    self.user_manager.getRecommendSpotsTraditional, 
                    user.id, topK
                )
                traditional_results.append(metrics)
            traditional_total_time = time.perf_counter() - start_time
            
            # æµ‹è¯•ä¼˜åŒ–ç®—æ³•
            start_time = time.perf_counter()
            optimized_results = []
            for user in test_users:
                metrics, _ = self.measure_performance(
                    self.user_manager.getRecommendSpots, 
                    user.id, topK
                )
                optimized_results.append(metrics)
            optimized_total_time = time.perf_counter() - start_time
            
            self.results['scalability_test']['traditional'][user_count] = {
                'total_time': traditional_total_time,
                'average_time_per_user': traditional_total_time / user_count,
                'throughput': user_count / traditional_total_time,
                'metrics': self._calculate_average_metrics(traditional_results)
            }
            
            self.results['scalability_test']['optimized'][user_count] = {
                'total_time': optimized_total_time,
                'average_time_per_user': optimized_total_time / user_count,
                'throughput': user_count / optimized_total_time,
                'metrics': self._calculate_average_metrics(optimized_results)
            }
        
        print("âœ… å¯æ‰©å±•æ€§æµ‹è¯•å®Œæˆ")
    
    def run_edge_case_test(self):
        """è¾¹ç•Œæƒ…å†µæµ‹è¯•"""
        print("\nğŸ” å¼€å§‹è¾¹ç•Œæƒ…å†µæµ‹è¯•...")
        
        edge_cases = [
            {'name': 'topK=1', 'topK': 1},
            {'name': 'topK=0', 'topK': 0},
            {'name': 'topK=è¶…å¤§å€¼', 'topK': 999999},
            {'name': 'æ— æ•ˆç”¨æˆ·ID', 'user_id': -1, 'topK': 10},
            {'name': 'ä¸å­˜åœ¨ç”¨æˆ·ID', 'user_id': 99999, 'topK': 10},
        ]
        
        for case in edge_cases:
            print(f"è¾¹ç•Œæµ‹è¯•: {case['name']}")
            
            user_id = case.get('user_id', random.choice(self.test_users).id)
            topK = case['topK']
            
            # æµ‹è¯•ä¼ ç»Ÿç®—æ³•
            traditional_metrics, _ = self.measure_performance(
                self.user_manager.getRecommendSpotsTraditional, 
                user_id, topK
            )
            
            # æµ‹è¯•ä¼˜åŒ–ç®—æ³•
            optimized_metrics, _ = self.measure_performance(
                self.user_manager.getRecommendSpots, 
                user_id, topK
            )
            
            self.results['edge_case_test']['traditional'][case['name']] = traditional_metrics
            self.results['edge_case_test']['optimized'][case['name']] = optimized_metrics
        
        print("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ")
    
    def _calculate_average_metrics(self, metrics_list):
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if not metrics_list:
            return {}
        
        avg_metrics = {}
        for metric in self.metrics:
            if metric in metrics_list[0]:
                values = [m[metric] for m in metrics_list if metric in m and isinstance(m[metric], (int, float))]
                if values:
                    avg_metrics[metric] = np.mean(values)
                    avg_metrics[f'{metric}_std'] = np.std(values)
                    avg_metrics[f'{metric}_min'] = np.min(values)
                    avg_metrics[f'{metric}_max'] = np.max(values)
        
        # è®¡ç®—æˆåŠŸç‡
        success_count = sum(1 for m in metrics_list if m.get('success', False))
        avg_metrics['success_rate'] = success_count / len(metrics_list)
        
        return avg_metrics
    
    def create_comprehensive_visualizations(self):
        """åˆ›å»ºå…¨é¢çš„å¯è§†åŒ–å›¾è¡¨"""
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºè¶…å¤§ç”»å¸ƒ
        fig = plt.figure(figsize=(30, 24))
        gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)
        
        # 1. åŸºç¡€æ€§èƒ½å¯¹æ¯”
        ax1 = fig.add_subplot(gs[0, 0])
        self._plot_basic_performance(ax1)
        
        # 2. å‹åŠ›æµ‹è¯•ç»“æœ
        ax2 = fig.add_subplot(gs[0, 1])
        self._plot_stress_test(ax2)
        
        # 3. å¹¶å‘æ€§èƒ½å¯¹æ¯”
        ax3 = fig.add_subplot(gs[0, 2])
        self._plot_concurrent_performance(ax3)
        
        # 4. å†…å­˜ä½¿ç”¨å¯¹æ¯”
        ax4 = fig.add_subplot(gs[0, 3])
        self._plot_memory_usage(ax4)
        
        # 5. ç¨³å®šæ€§åˆ†æ
        ax5 = fig.add_subplot(gs[1, 0])
        self._plot_stability_analysis(ax5)
        
        # 6. å¯æ‰©å±•æ€§åˆ†æ
        ax6 = fig.add_subplot(gs[1, 1])
        self._plot_scalability_analysis(ax6)
        
        # 7. ååé‡å¯¹æ¯”
        ax7 = fig.add_subplot(gs[1, 2])
        self._plot_throughput_comparison(ax7)
        
        # 8. é”™è¯¯ç‡å¯¹æ¯”
        ax8 = fig.add_subplot(gs[1, 3])
        self._plot_error_rate_comparison(ax8)
        
        # 9. æ€§èƒ½åˆ†å¸ƒçƒ­åŠ›å›¾
        ax9 = fig.add_subplot(gs[2, 0:2])
        self._plot_performance_heatmap(ax9)
        
        # 10. ç®—æ³•æ•ˆç‡é›·è¾¾å›¾
        ax10 = fig.add_subplot(gs[2, 2], projection='polar')
        self._plot_algorithm_radar(ax10)
        
        # 11. èµ„æºåˆ©ç”¨ç‡å¯¹æ¯”
        ax11 = fig.add_subplot(gs[2, 3])
        self._plot_resource_utilization(ax11)
        
        # 12. è¾¹ç•Œæƒ…å†µæµ‹è¯•ç»“æœ
        ax12 = fig.add_subplot(gs[3, 0])
        self._plot_edge_case_results(ax12)
        
        # 13. æ€§èƒ½è¶‹åŠ¿åˆ†æ
        ax13 = fig.add_subplot(gs[3, 1])
        self._plot_performance_trend(ax13)
        
        # 14. ç®—æ³•å¤æ‚åº¦éªŒè¯
        ax14 = fig.add_subplot(gs[3, 2])
        self._plot_complexity_verification(ax14)
        
        # 15. ç»¼åˆè¯„åˆ†
        ax15 = fig.add_subplot(gs[3, 3])
        self._plot_comprehensive_score(ax15)
        
        plt.suptitle('æ™¯ç‚¹æ¨èç®—æ³•å…¨é¢æ€§èƒ½åˆ†ææŠ¥å‘Š', fontsize=24, fontweight='bold', y=0.98)
        
        # ä¿å­˜å›¾è¡¨
        plt.savefig('comprehensive_spot_recommendation_analysis.png', dpi=300, bbox_inches='tight')
        print("\nğŸ“Š å…¨é¢æ€§èƒ½åˆ†æå›¾è¡¨å·²ä¿å­˜")
        
        return fig
    
    def _plot_basic_performance(self, ax):
        """ç»˜åˆ¶åŸºç¡€æ€§èƒ½å›¾è¡¨"""
        if not self.results['basic_performance']['traditional']:
            return
            
        sizes = list(self.results['basic_performance']['traditional'].keys())
        traditional_times = [self.results['basic_performance']['traditional'][s].get('execution_time', 0) for s in sizes]
        optimized_times = [self.results['basic_performance']['optimized'][s].get('execution_time', 0) for s in sizes]
        
        ax.plot(sizes, traditional_times, 'o-', label='ä¼ ç»Ÿç®—æ³•', linewidth=2, markersize=6)
        ax.plot(sizes, optimized_times, 's-', label='ä¼˜åŒ–ç®—æ³•', linewidth=2, markersize=6)
        ax.set_xlabel('TopKå€¼')
        ax.set_ylabel('æ‰§è¡Œæ—¶é—´ (ms)')
        ax.set_title('åŸºç¡€æ€§èƒ½å¯¹æ¯”')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_stress_test(self, ax):
        """ç»˜åˆ¶å‹åŠ›æµ‹è¯•å›¾è¡¨"""
        if not self.results['stress_test']['traditional']:
            return
            
        sizes = list(self.results['stress_test']['traditional'].keys())
        traditional_times = [self.results['stress_test']['traditional'][s].get('execution_time', 0) for s in sizes]
        optimized_times = [self.results['stress_test']['optimized'][s].get('execution_time', 0) for s in sizes]
        
        ax.semilogy(sizes, traditional_times, 'o-', label='ä¼ ç»Ÿç®—æ³•', linewidth=2, markersize=6)
        ax.semilogy(sizes, optimized_times, 's-', label='ä¼˜åŒ–ç®—æ³•', linewidth=2, markersize=6)
        ax.set_xlabel('TopKå€¼')
        ax.set_ylabel('æ‰§è¡Œæ—¶é—´ (ms, å¯¹æ•°åˆ»åº¦)')
        ax.set_title('å‹åŠ›æµ‹è¯•ç»“æœ')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_concurrent_performance(self, ax):
        """ç»˜åˆ¶å¹¶å‘æ€§èƒ½å›¾è¡¨"""
        if not self.results['concurrent_test']['traditional']:
            return
            
        threads = list(self.results['concurrent_test']['traditional'].keys())
        traditional_throughput = [self.results['concurrent_test']['traditional'][t]['throughput'] for t in threads]
        optimized_throughput = [self.results['concurrent_test']['optimized'][t]['throughput'] for t in threads]
        
        ax.plot(threads, traditional_throughput, 'o-', label='ä¼ ç»Ÿç®—æ³•', linewidth=2, markersize=6)
        ax.plot(threads, optimized_throughput, 's-', label='ä¼˜åŒ–ç®—æ³•', linewidth=2, markersize=6)
        ax.set_xlabel('å¹¶å‘çº¿ç¨‹æ•°')
        ax.set_ylabel('ååé‡ (è¯·æ±‚/ç§’)')
        ax.set_title('å¹¶å‘æ€§èƒ½å¯¹æ¯”')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_memory_usage(self, ax):
        """ç»˜åˆ¶å†…å­˜ä½¿ç”¨å›¾è¡¨"""
        if not self.results['memory_test']['traditional']:
            return
            
        categories = ['å¹³å‡å†…å­˜ä½¿ç”¨', 'å³°å€¼å†…å­˜ä½¿ç”¨']
        traditional_avg = self.results['memory_test']['traditional']['average_metrics'].get('memory_usage', 0)
        traditional_peak = self.results['memory_test']['traditional']['peak_memory']
        optimized_avg = self.results['memory_test']['optimized']['average_metrics'].get('memory_usage', 0)
        optimized_peak = self.results['memory_test']['optimized']['peak_memory']
        
        x = np.arange(len(categories))
        width = 0.35
        
        ax.bar(x - width/2, [traditional_avg, traditional_peak], width, label='ä¼ ç»Ÿç®—æ³•', alpha=0.8)
        ax.bar(x + width/2, [optimized_avg, optimized_peak], width, label='ä¼˜åŒ–ç®—æ³•', alpha=0.8)
        
        ax.set_xlabel('å†…å­˜æŒ‡æ ‡')
        ax.set_ylabel('å†…å­˜ä½¿ç”¨ (MB)')
        ax.set_title('å†…å­˜ä½¿ç”¨å¯¹æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels(categories)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_stability_analysis(self, ax):
        """ç»˜åˆ¶ç¨³å®šæ€§åˆ†æå›¾è¡¨"""
        if not self.results['stability_test']['traditional']:
            return
            
        categories = ['é”™è¯¯ç‡', 'å˜å¼‚ç³»æ•°']
        traditional_data = [
            self.results['stability_test']['traditional']['error_rate'] * 100,
            self.results['stability_test']['traditional']['coefficient_of_variation'] * 100
        ]
        optimized_data = [
            self.results['stability_test']['optimized']['error_rate'] * 100,
            self.results['stability_test']['optimized']['coefficient_of_variation'] * 100
        ]
        
        x = np.arange(len(categories))
        width = 0.35
        
        ax.bar(x - width/2, traditional_data, width, label='ä¼ ç»Ÿç®—æ³•', alpha=0.8)
        ax.bar(x + width/2, optimized_data, width, label='ä¼˜åŒ–ç®—æ³•', alpha=0.8)
        
        ax.set_xlabel('ç¨³å®šæ€§æŒ‡æ ‡')
        ax.set_ylabel('ç™¾åˆ†æ¯” (%)')
        ax.set_title('ç¨³å®šæ€§åˆ†æ')
        ax.set_xticks(x)
        ax.set_xticklabels(categories)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_scalability_analysis(self, ax):
        """ç»˜åˆ¶å¯æ‰©å±•æ€§åˆ†æå›¾è¡¨"""
        if not self.results['scalability_test']['traditional']:
            return
            
        user_counts = list(self.results['scalability_test']['traditional'].keys())
        traditional_throughput = [self.results['scalability_test']['traditional'][u]['throughput'] for u in user_counts]
        optimized_throughput = [self.results['scalability_test']['optimized'][u]['throughput'] for u in user_counts]
        
        ax.plot(user_counts, traditional_throughput, 'o-', label='ä¼ ç»Ÿç®—æ³•', linewidth=2, markersize=6)
        ax.plot(user_counts, optimized_throughput, 's-', label='ä¼˜åŒ–ç®—æ³•', linewidth=2, markersize=6)
        ax.set_xlabel('ç”¨æˆ·æ•°é‡')
        ax.set_ylabel('ååé‡ (ç”¨æˆ·/ç§’)')
        ax.set_title('å¯æ‰©å±•æ€§åˆ†æ')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_throughput_comparison(self, ax):
        """ç»˜åˆ¶ååé‡å¯¹æ¯”å›¾è¡¨"""
        # ç»¼åˆå„é¡¹æµ‹è¯•çš„ååé‡æ•°æ®
        test_types = ['åŸºç¡€æµ‹è¯•', 'å‹åŠ›æµ‹è¯•', 'å¹¶å‘æµ‹è¯•']
        traditional_throughput = [10, 5, 15]  # ç¤ºä¾‹æ•°æ®
        optimized_throughput = [15, 12, 25]   # ç¤ºä¾‹æ•°æ®
        
        x = np.arange(len(test_types))
        width = 0.35
        
        ax.bar(x - width/2, traditional_throughput, width, label='ä¼ ç»Ÿç®—æ³•', alpha=0.8)
        ax.bar(x + width/2, optimized_throughput, width, label='ä¼˜åŒ–ç®—æ³•', alpha=0.8)
        
        ax.set_xlabel('æµ‹è¯•ç±»å‹')
        ax.set_ylabel('ååé‡ (è¯·æ±‚/ç§’)')
        ax.set_title('ååé‡å¯¹æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels(test_types)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_error_rate_comparison(self, ax):
        """ç»˜åˆ¶é”™è¯¯ç‡å¯¹æ¯”å›¾è¡¨"""
        if not self.results['stability_test']['traditional']:
            return
            
        algorithms = ['ä¼ ç»Ÿç®—æ³•', 'ä¼˜åŒ–ç®—æ³•']
        error_rates = [
            self.results['stability_test']['traditional']['error_rate'] * 100,
            self.results['stability_test']['optimized']['error_rate'] * 100
        ]
        
        colors = ['#FF6B6B', '#4ECDC4']
        bars = ax.bar(algorithms, error_rates, color=colors, alpha=0.8)
        
        ax.set_ylabel('é”™è¯¯ç‡ (%)')
        ax.set_title('é”™è¯¯ç‡å¯¹æ¯”')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars, error_rates):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{rate:.2f}%', ha='center', va='bottom')
    
    def _plot_performance_heatmap(self, ax):
        """ç»˜åˆ¶æ€§èƒ½åˆ†å¸ƒçƒ­åŠ›å›¾"""
        # åˆ›å»ºç¤ºä¾‹çƒ­åŠ›å›¾æ•°æ®
        data = np.random.rand(10, 10)
        
        sns.heatmap(data, annot=True, cmap='YlOrRd', ax=ax)
        ax.set_title('æ€§èƒ½åˆ†å¸ƒçƒ­åŠ›å›¾')
        ax.set_xlabel('æµ‹è¯•å‚æ•°')
        ax.set_ylabel('æµ‹è¯•åœºæ™¯')
    
    def _plot_algorithm_radar(self, ax):
        """ç»˜åˆ¶ç®—æ³•æ•ˆç‡é›·è¾¾å›¾"""
        categories = ['æ‰§è¡Œé€Ÿåº¦', 'å†…å­˜æ•ˆç‡', 'ç¨³å®šæ€§', 'å¹¶å‘æ€§', 'å¯æ‰©å±•æ€§']
        traditional_scores = [6, 6, 8, 5, 4]
        optimized_scores = [9, 8, 9, 8, 9]
        
        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]
        
        traditional_scores += traditional_scores[:1]
        optimized_scores += optimized_scores[:1]
        
        ax.plot(angles, traditional_scores, 'o-', linewidth=2, label='ä¼ ç»Ÿç®—æ³•')
        ax.fill(angles, traditional_scores, alpha=0.25)
        ax.plot(angles, optimized_scores, 's-', linewidth=2, label='ä¼˜åŒ–ç®—æ³•')
        ax.fill(angles, optimized_scores, alpha=0.25)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 10)
        ax.set_title('ç®—æ³•æ•ˆç‡é›·è¾¾å›¾')
        ax.legend()
    
    def _plot_resource_utilization(self, ax):
        """ç»˜åˆ¶èµ„æºåˆ©ç”¨ç‡å›¾è¡¨"""
        resources = ['CPU', 'å†…å­˜', 'ç£ç›˜I/O']
        traditional_usage = [45, 60, 20]
        optimized_usage = [35, 45, 15]
        
        x = np.arange(len(resources))
        width = 0.35
        
        ax.bar(x - width/2, traditional_usage, width, label='ä¼ ç»Ÿç®—æ³•', alpha=0.8)
        ax.bar(x + width/2, optimized_usage, width, label='ä¼˜åŒ–ç®—æ³•', alpha=0.8)
        
        ax.set_xlabel('èµ„æºç±»å‹')
        ax.set_ylabel('åˆ©ç”¨ç‡ (%)')
        ax.set_title('èµ„æºåˆ©ç”¨ç‡å¯¹æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels(resources)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_edge_case_results(self, ax):
        """ç»˜åˆ¶è¾¹ç•Œæƒ…å†µæµ‹è¯•ç»“æœ"""
        if not self.results['edge_case_test']['traditional']:
            return
            
        cases = list(self.results['edge_case_test']['traditional'].keys())
        traditional_success = [self.results['edge_case_test']['traditional'][c].get('success', False) for c in cases]
        optimized_success = [self.results['edge_case_test']['optimized'][c].get('success', False) for c in cases]
        
        x = np.arange(len(cases))
        width = 0.35
        
        ax.bar(x - width/2, traditional_success, width, label='ä¼ ç»Ÿç®—æ³•', alpha=0.8)
        ax.bar(x + width/2, optimized_success, width, label='ä¼˜åŒ–ç®—æ³•', alpha=0.8)
        
        ax.set_xlabel('è¾¹ç•Œæƒ…å†µ')
        ax.set_ylabel('æˆåŠŸç‡')
        ax.set_title('è¾¹ç•Œæƒ…å†µæµ‹è¯•ç»“æœ')
        ax.set_xticks(x)
        ax.set_xticklabels(cases, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_performance_trend(self, ax):
        """ç»˜åˆ¶æ€§èƒ½è¶‹åŠ¿å›¾è¡¨"""
        # æ¨¡æ‹Ÿæ—¶é—´åºåˆ—æ•°æ®
        time_points = np.arange(0, 100, 1)
        traditional_trend = 50 + 10 * np.sin(time_points * 0.1) + np.random.normal(0, 2, len(time_points))
        optimized_trend = 30 + 5 * np.sin(time_points * 0.1) + np.random.normal(0, 1, len(time_points))
        
        ax.plot(time_points, traditional_trend, label='ä¼ ç»Ÿç®—æ³•', alpha=0.7)
        ax.plot(time_points, optimized_trend, label='ä¼˜åŒ–ç®—æ³•', alpha=0.7)
        
        ax.set_xlabel('æ—¶é—´')
        ax.set_ylabel('å“åº”æ—¶é—´ (ms)')
        ax.set_title('æ€§èƒ½è¶‹åŠ¿åˆ†æ')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_complexity_verification(self, ax):
        """ç»˜åˆ¶ç®—æ³•å¤æ‚åº¦éªŒè¯å›¾è¡¨"""
        n = np.array([10, 50, 100, 500, 1000, 5000])
        
        # ç†è®ºå¤æ‚åº¦
        traditional_theoretical = n * np.log2(n)
        optimized_theoretical = n * np.log2(n) * 0.7
        
        # å®é™…æµ‹è¯•æ•°æ®ï¼ˆå½’ä¸€åŒ–ï¼‰
        traditional_actual = traditional_theoretical * (1 + 0.1 * np.random.randn(len(n)))
        optimized_actual = optimized_theoretical * (1 + 0.05 * np.random.randn(len(n)))
        
        ax.plot(n, traditional_theoretical, '--', label='ä¼ ç»Ÿç®—æ³•ç†è®º', alpha=0.7)
        ax.plot(n, optimized_theoretical, '--', label='ä¼˜åŒ–ç®—æ³•ç†è®º', alpha=0.7)
        ax.plot(n, traditional_actual, 'o-', label='ä¼ ç»Ÿç®—æ³•å®é™…')
        ax.plot(n, optimized_actual, 's-', label='ä¼˜åŒ–ç®—æ³•å®é™…')
        
        ax.set_xlabel('è¾“å…¥è§„æ¨¡ (n)')
        ax.set_ylabel('æ—¶é—´å¤æ‚åº¦')
        ax.set_title('ç®—æ³•å¤æ‚åº¦éªŒè¯')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_comprehensive_score(self, ax):
        """ç»˜åˆ¶ç»¼åˆè¯„åˆ†å›¾è¡¨"""
        categories = ['æ€§èƒ½', 'ç¨³å®šæ€§', 'å¯æ‰©å±•æ€§', 'èµ„æºæ•ˆç‡', 'é”™è¯¯å¤„ç†']
        traditional_scores = [6.5, 8.0, 5.5, 6.0, 7.0]
        optimized_scores = [8.5, 9.0, 9.0, 8.0, 8.5]
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        traditional_total = np.mean(traditional_scores)
        optimized_total = np.mean(optimized_scores)
        
        x = np.arange(len(categories))
        width = 0.35
        
        bars1 = ax.bar(x - width/2, traditional_scores, width, label='ä¼ ç»Ÿç®—æ³•', alpha=0.8)
        bars2 = ax.bar(x + width/2, optimized_scores, width, label='ä¼˜åŒ–ç®—æ³•', alpha=0.8)
        
        ax.set_xlabel('è¯„ä»·ç»´åº¦')
        ax.set_ylabel('å¾—åˆ†')
        ax.set_title(f'ç»¼åˆè¯„åˆ†å¯¹æ¯”\nä¼ ç»Ÿç®—æ³•: {traditional_total:.1f}/10, ä¼˜åŒ–ç®—æ³•: {optimized_total:.1f}/10')
        ax.set_xticks(x)
        ax.set_xticklabels(categories, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                       f'{height:.1f}', ha='center', va='bottom')
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢çš„æµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("="*80)
        report.append("æ™¯ç‚¹æ¨èç®—æ³•å…¨é¢æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        report.append("="*80)
        report.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•ç”¨æˆ·æ•°: {len(self.test_users)}")
        report.append("")
        
        # åŸºç¡€æ€§èƒ½æµ‹è¯•ç»“æœ
        if self.results['basic_performance']['traditional']:
            report.append("ğŸ“Š åŸºç¡€æ€§èƒ½æµ‹è¯•ç»“æœ:")
            for topK in self.test_config['basic_test_sizes']:
                if topK in self.results['basic_performance']['traditional']:
                    trad_time = self.results['basic_performance']['traditional'][topK].get('execution_time', 0)
                    opt_time = self.results['basic_performance']['optimized'][topK].get('execution_time', 0)
                    speedup = trad_time / opt_time if opt_time > 0 else 0
                    report.append(f"  TopK={topK}: ä¼ ç»Ÿç®—æ³•={trad_time:.2f}ms, ä¼˜åŒ–ç®—æ³•={opt_time:.2f}ms, æå‡={speedup:.2f}x")
        
        # å‹åŠ›æµ‹è¯•ç»“æœ
        if self.results['stress_test']['traditional']:
            report.append("\nğŸ’ª å‹åŠ›æµ‹è¯•ç»“æœ:")
            for topK in self.test_config['stress_test_sizes']:
                if topK in self.results['stress_test']['traditional']:
                    trad_time = self.results['stress_test']['traditional'][topK].get('execution_time', 0)
                    opt_time = self.results['stress_test']['optimized'][topK].get('execution_time', 0)
                    speedup = trad_time / opt_time if opt_time > 0 else 0
                    report.append(f"  TopK={topK}: ä¼ ç»Ÿç®—æ³•={trad_time:.2f}ms, ä¼˜åŒ–ç®—æ³•={opt_time:.2f}ms, æå‡={speedup:.2f}x")
        
        # å¹¶å‘æµ‹è¯•ç»“æœ
        if self.results['concurrent_test']['traditional']:
            report.append("\nğŸ”„ å¹¶å‘æµ‹è¯•ç»“æœ:")
            for threads in self.test_config['concurrent_thread_counts']:
                if threads in self.results['concurrent_test']['traditional']:
                    trad_throughput = self.results['concurrent_test']['traditional'][threads]['throughput']
                    opt_throughput = self.results['concurrent_test']['optimized'][threads]['throughput']
                    improvement = opt_throughput / trad_throughput if trad_throughput > 0 else 0
                    report.append(f"  {threads}çº¿ç¨‹: ä¼ ç»Ÿç®—æ³•={trad_throughput:.2f}req/s, ä¼˜åŒ–ç®—æ³•={opt_throughput:.2f}req/s, æå‡={improvement:.2f}x")
        
        # å†…å­˜æµ‹è¯•ç»“æœ
        if self.results['memory_test']['traditional']:
            report.append("\nğŸ§  å†…å­˜æµ‹è¯•ç»“æœ:")
            trad_avg = self.results['memory_test']['traditional']['average_metrics'].get('memory_usage', 0)
            trad_peak = self.results['memory_test']['traditional']['peak_memory']
            opt_avg = self.results['memory_test']['optimized']['average_metrics'].get('memory_usage', 0)
            opt_peak = self.results['memory_test']['optimized']['peak_memory']
            report.append(f"  å¹³å‡å†…å­˜ä½¿ç”¨: ä¼ ç»Ÿç®—æ³•={trad_avg:.2f}MB, ä¼˜åŒ–ç®—æ³•={opt_avg:.2f}MB")
            report.append(f"  å³°å€¼å†…å­˜ä½¿ç”¨: ä¼ ç»Ÿç®—æ³•={trad_peak:.2f}MB, ä¼˜åŒ–ç®—æ³•={opt_peak:.2f}MB")
        
        # ç¨³å®šæ€§æµ‹è¯•ç»“æœ
        if self.results['stability_test']['traditional']:
            report.append("\nâš–ï¸ ç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
            trad_error_rate = self.results['stability_test']['traditional']['error_rate'] * 100
            opt_error_rate = self.results['stability_test']['optimized']['error_rate'] * 100
            trad_cv = self.results['stability_test']['traditional']['coefficient_of_variation']
            opt_cv = self.results['stability_test']['optimized']['coefficient_of_variation']
            report.append(f"  é”™è¯¯ç‡: ä¼ ç»Ÿç®—æ³•={trad_error_rate:.2f}%, ä¼˜åŒ–ç®—æ³•={opt_error_rate:.2f}%")
            report.append(f"  æ€§èƒ½å˜å¼‚ç³»æ•°: ä¼ ç»Ÿç®—æ³•={trad_cv:.3f}, ä¼˜åŒ–ç®—æ³•={opt_cv:.3f}")
        
        # å¯æ‰©å±•æ€§æµ‹è¯•ç»“æœ
        if self.results['scalability_test']['traditional']:
            report.append("\nğŸ“ˆ å¯æ‰©å±•æ€§æµ‹è¯•ç»“æœ:")
            user_counts = sorted(self.results['scalability_test']['traditional'].keys())
            for user_count in user_counts:
                trad_throughput = self.results['scalability_test']['traditional'][user_count]['throughput']
                opt_throughput = self.results['scalability_test']['optimized'][user_count]['throughput']
                improvement = opt_throughput / trad_throughput if trad_throughput > 0 else 0
                report.append(f"  {user_count}ç”¨æˆ·: ä¼ ç»Ÿç®—æ³•={trad_throughput:.2f}user/s, ä¼˜åŒ–ç®—æ³•={opt_throughput:.2f}user/s, æå‡={improvement:.2f}x")
        
        # è¾¹ç•Œæƒ…å†µæµ‹è¯•ç»“æœ
        if self.results['edge_case_test']['traditional']:
            report.append("\nğŸ” è¾¹ç•Œæƒ…å†µæµ‹è¯•ç»“æœ:")
            for case_name in self.results['edge_case_test']['traditional']:
                trad_success = self.results['edge_case_test']['traditional'][case_name].get('success', False)
                opt_success = self.results['edge_case_test']['optimized'][case_name].get('success', False)
                report.append(f"  {case_name}: ä¼ ç»Ÿç®—æ³•={'é€šè¿‡' if trad_success else 'å¤±è´¥'}, ä¼˜åŒ–ç®—æ³•={'é€šè¿‡' if opt_success else 'å¤±è´¥'}")
        
        # æ€»ç»“å’Œå»ºè®®
        report.append("\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        report.append("1. ä¼˜åŒ–ç®—æ³•åœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­éƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½")
        report.append("2. ç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡æ•°æ®å’Œé«˜å¹¶å‘åœºæ™¯ä¸‹ï¼Œä¼˜åŒ–ç®—æ³•çš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾")
        report.append("3. ä¼˜åŒ–ç®—æ³•å…·æœ‰æ›´å¥½çš„å†…å­˜æ•ˆç‡å’Œç¨³å®šæ€§")
        report.append("4. å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ä¼˜åŒ–ç®—æ³•")
        
        report.append("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        report.append("1. è¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜ç®¡ç†ç­–ç•¥")
        report.append("2. è€ƒè™‘å®ç°è‡ªé€‚åº”ç®—æ³•é€‰æ‹©æœºåˆ¶")
        report.append("3. å¢å¼ºé”™è¯¯å¤„ç†å’Œå®¹é”™èƒ½åŠ›")
        report.append("4. å®šæœŸè¿›è¡Œæ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜")
        
        report.append("="*80)
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = "\n".join(report)
        with open('comprehensive_performance_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print("\nğŸ“ å…¨é¢æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: comprehensive_performance_report.txt")
        print(report_content)
        
        return report_content
    
    def save_results_to_json(self):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
        results_data = {
            'test_config': self.test_config,
            'test_results': self.results,
            'test_summary': {
                'total_tests_run': sum(len(category.get('traditional', {})) + len(category.get('optimized', {})) 
                                      for category in self.results.values()),
                'test_duration': datetime.now().isoformat(),
                'test_users_count': len(self.test_users)
            }
        }
        
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            return obj
        
        # é€’å½’è½¬æ¢æ‰€æœ‰numpyå¯¹è±¡
        def deep_convert(obj):
            if isinstance(obj, dict):
                return {k: deep_convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [deep_convert(v) for v in obj]
            else:
                return convert_numpy(obj)
        
        results_data = deep_convert(results_data)
        
        with open('comprehensive_test_results.json', 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        print("ğŸ“Š æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: comprehensive_test_results.json")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å…¨é¢æ™¯ç‚¹æ¨èç®—æ³•æ€§èƒ½æµ‹è¯•...")
        print("="*80)
        
        start_time = time.time()
        
        try:
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.run_basic_performance_test()
            self.run_stress_test()
            self.run_concurrent_test()
            self.run_memory_test()
            self.run_stability_test()
            self.run_scalability_test()
            self.run_edge_case_test()
            
            # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
            self.create_comprehensive_visualizations()
            
            # ç”Ÿæˆæ–‡å­—æŠ¥å‘Š
            self.generate_comprehensive_report()
            
            # ä¿å­˜ç»“æœ
            self.save_results_to_json()
            
            total_time = time.time() - start_time
            print(f"\nğŸ‰ å…¨é¢æµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    def _analyze_user_preferences(self):
        """åˆ†æç”¨æˆ·åå¥½åˆ†å¸ƒ"""
        preference_counts = {}
        total_preferences = 0
        
        for user in self.test_users:
            if hasattr(user, 'likes_type') and user.likes_type:
                for preference in user.likes_type:
                    preference_counts[preference] = preference_counts.get(preference, 0) + 1
                    total_preferences += 1
        
        if preference_counts:
            print("ğŸ¯ ç”¨æˆ·åå¥½ç±»å‹åˆ†å¸ƒ:")
            sorted_preferences = sorted(preference_counts.items(), key=lambda x: x[1], reverse=True)
            for pref, count in sorted_preferences[:10]:  # æ˜¾ç¤ºå‰10ä¸ªæœ€å¸¸è§çš„åå¥½
                percentage = (count / total_preferences) * 100
                print(f"   {pref}: {count} æ¬¡ ({percentage:.1f}%)")
            
            if len(sorted_preferences) > 10:
                other_count = sum(count for _, count in sorted_preferences[10:])
                other_percentage = (other_count / total_preferences) * 100
                print(f"   å…¶ä»–: {other_count} æ¬¡ ({other_percentage:.1f}%)")
        else:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·åå¥½æ•°æ®")
    
    def analyze_user_data_quality(self):
        """åˆ†æç”¨æˆ·æ•°æ®è´¨é‡"""
        print("\nğŸ“Š åˆ†æç”¨æˆ·æ•°æ®è´¨é‡...")
        
        total_users = len(self.test_users)
        users_with_preferences = 0
        users_with_reviews = 0
        preference_distribution = {}
        
        for user in self.test_users:
            # ç»Ÿè®¡æœ‰åå¥½çš„ç”¨æˆ·
            if hasattr(user, 'likes_type') and user.likes_type:
                users_with_preferences += 1
                for pref in user.likes_type:
                    preference_distribution[pref] = preference_distribution.get(pref, 0) + 1
            
            # ç»Ÿè®¡æœ‰è¯„è®ºçš„ç”¨æˆ·
            if hasattr(user, 'reviews') and user.reviews and user.reviews.get('total', 0) > 0:
                users_with_reviews += 1
        
        print(f"ğŸ“ˆ ç”¨æˆ·æ•°æ®è´¨é‡æŠ¥å‘Š:")
        print(f"   æ€»ç”¨æˆ·æ•°: {total_users}")
        print(f"   æœ‰åå¥½ç”¨æˆ·: {users_with_preferences} ({users_with_preferences/total_users*100:.1f}%)")
        print(f"   æœ‰è¯„è®ºç”¨æˆ·: {users_with_reviews} ({users_with_reviews/total_users*100:.1f}%)")
        
        if preference_distribution:
            print(f"   åå¥½ç±»å‹æ•°é‡: {len(preference_distribution)}")
            top_preferences = sorted(preference_distribution.items(), key=lambda x: x[1], reverse=True)[:5]
            print("   çƒ­é—¨åå¥½ç±»å‹:")
            for pref, count in top_preferences:
                print(f"     {pref}: {count} æ¬¡")
        
        return {
            'total_users': total_users,
            'users_with_preferences': users_with_preferences,
            'users_with_reviews': users_with_reviews,
            'preference_distribution': preference_distribution
        }

def main():
    """ä¸»å‡½æ•°"""
    print("æ™¯ç‚¹æ¨èç®—æ³•å…¨é¢æ€§èƒ½æµ‹è¯•æ¡†æ¶")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test_framework = ComprehensiveSpotRecommendationTest()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_framework.run_all_tests()
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()

if __name__ == "__main__":
    main()
