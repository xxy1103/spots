# -*- coding: utf-8 -*-
"""
æ¨èæ™¯åŒºç³»ç»Ÿè¿è¡Œæ—¶é—´æ€§èƒ½æµ‹è¯•ç¨‹åº - ä¸­æ–‡å­—ä½“ä¿®å¤ç‰ˆ
ä¸“é—¨æµ‹è¯•æ¨èç³»ç»Ÿå„é¡¹æ“ä½œçš„å“åº”æ—¶é—´å’Œæ€§èƒ½è¡¨ç°
åŒ…æ‹¬å¤šç§ç°å®æƒ…å½¢ä¸‹çš„æµ‹è¯•ï¼Œæä¾›è¯¦ç»†çš„è¿è¡Œæ—¶é—´æ•°æ®åˆ†æ
"""

import sys
import os
import time
import random
import statistics
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import pandas as pd
import seaborn as sns
from datetime import datetime
from collections import defaultdict
import json

# è®¾ç½®ä¸­æ–‡å­—ä½“ - æœ€ç»ˆä¿®å¤ç‰ˆ
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ - ä½¿ç”¨æœ€æœ‰æ•ˆçš„é…ç½®æ–¹æ¡ˆ"""
    print("ğŸ”§ è®¾ç½®æœ€ç»ˆç‰ˆä¸­æ–‡å­—ä½“é…ç½®...")
    
    # ç›´æ¥è®¾ç½®æœ€å¯é çš„é…ç½®
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['axes.unicode_minus'] = False
    
    # å¼ºåˆ¶å­—ä½“ç¼“å­˜åˆ·æ–°
    try:
        fm._rebuild()
        print("âœ… å­—ä½“ç¼“å­˜å·²åˆ·æ–°")
    except Exception as e:
        print(f"âš ï¸ å­—ä½“ç¼“å­˜åˆ·æ–°è­¦å‘Š: {e}")
    
    # éªŒè¯ä¸­æ–‡å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    chinese_fonts = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei']
    
    found_font = None
    for font in chinese_fonts:
        if font in available_fonts:
            found_font = font
            break
    
    if found_font:
        print(f"âœ… æ‰¾åˆ°ä¸­æ–‡å­—ä½“: {found_font}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ä¸“ç”¨ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤")
    
    return found_font

def ensure_chinese_display():
    """ç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸ - åœ¨æ¯æ¬¡ç»˜å›¾å‰è°ƒç”¨"""
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

# åˆå§‹åŒ–å­—ä½“è®¾ç½®
selected_chinese_font = setup_chinese_font()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from module.user_class import UserManager, userManager
from module.Spot_class import spotManager


class SpotRecommendationRuntimeTest:
    """æ¨èæ™¯åŒºç³»ç»Ÿè¿è¡Œæ—¶é—´æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.user_manager = userManager
        self.spot_manager = spotManager
        
        # æµ‹è¯•é…ç½®
        self.test_configs = {
            'topK_values': [5, 10, 15, 20, 25, 30, 40, 50],  # ä¸åŒçš„æ¨èæ•°é‡
            'repeat_times': 15,  # æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°ï¼Œç”¨äºè®¡ç®—å¹³å‡å€¼
            'user_sample_size': 25,  # æµ‹è¯•ç”¨æˆ·æ ·æœ¬å¤§å°
            'timeout_threshold': 10000,  # è¶…æ—¶é˜ˆå€¼(ms)
            'performance_levels': {
                'excellent': 50,    # ä¼˜ç§€ï¼šâ‰¤50ms
                'good': 100,        # è‰¯å¥½ï¼š51-100ms
                'acceptable': 200,  # å¯æ¥å—ï¼š101-200ms
                'slow': 500,        # è¾ƒæ…¢ï¼š201-500ms
                'very_slow': float('inf')  # å¾ˆæ…¢ï¼š>500ms
            }
        }
        
        # ç”¨æˆ·åœºæ™¯åˆ†ç±»
        self.user_scenarios = {
            'single_preference': {'preference_count': 1, 'description': 'å•ä¸€åå¥½ç”¨æˆ·'},
            'dual_preference': {'preference_count': 2, 'description': 'åŒé‡åå¥½ç”¨æˆ·'},
            'multi_preference': {'preference_count': 3, 'description': 'å¤šé‡åå¥½ç”¨æˆ·'},
            'diverse_preference': {'preference_count': 4, 'description': 'å¤šæ ·åŒ–åå¥½ç”¨æˆ·'}
        }
        
        # å­˜å‚¨æµ‹è¯•ç»“æœ
        self.test_results = {
            'basic_performance': {},
            'scenario_analysis': {},
            'load_testing': {},
            'stability_testing': {},
            'comprehensive_analysis': {}
        }
        self.detailed_records = []  # è¯¦ç»†æµ‹è¯•è®°å½•
        self.test_users = []
        
        print("ğŸš€ æ¨èæ™¯åŒºç³»ç»Ÿè¿è¡Œæ—¶é—´æ€§èƒ½æµ‹è¯•ç¨‹åºå¯åŠ¨", flush=True)
        self._prepare_test_environment()
    
    def _prepare_test_environment(self):
        """å‡†å¤‡æµ‹è¯•ç¯å¢ƒ"""
        print("\nğŸ”§ å‡†å¤‡æµ‹è¯•ç¯å¢ƒ...", flush=True)
        
        # è·å–å¯ç”¨çš„æµ‹è¯•ç”¨æˆ·
        available_users = [user for user in self.user_manager.users 
                          if hasattr(user, 'likes_type') and user.likes_type]
        
        if len(available_users) < self.test_configs['user_sample_size']:
            print(f"âš ï¸  å¯ç”¨ç”¨æˆ·æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨ç”¨æˆ· ({len(available_users)}ä¸ª)")
            self.test_users = available_users
        else:
            self.test_users = random.sample(available_users, self.test_configs['user_sample_size'])
        
        print(f"âœ… æµ‹è¯•ç¯å¢ƒå‡†å¤‡å®Œæˆï¼Œå…±{len(self.test_users)}ä¸ªæµ‹è¯•ç”¨æˆ·")
        self._analyze_test_users()
    
    def _analyze_test_users(self):
        """åˆ†ææµ‹è¯•ç”¨æˆ·åˆ†å¸ƒ"""
        print("\nğŸ“Š æµ‹è¯•ç”¨æˆ·åˆ†å¸ƒåˆ†æ:")
        
        preference_distribution = defaultdict(int)
        total_preferences = 0
        
        for user in self.test_users:
            pref_count = len(user.likes_type) if user.likes_type else 0
            preference_distribution[pref_count] += 1
            total_preferences += pref_count
        
        for count, num_users in sorted(preference_distribution.items()):
            percentage = (num_users / len(self.test_users)) * 100
            print(f"   {count}ä¸ªåå¥½: {num_users}ç”¨æˆ· ({percentage:.1f}%)")
        
        avg_preferences = total_preferences / len(self.test_users) if self.test_users else 0
        print(f"   å¹³å‡åå¥½æ•°é‡: {avg_preferences:.2f}")
    
    def measure_execution_time(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        start_time = time.perf_counter()
        
        try:
            result = func(*args, **kwargs)
            end_time = time.perf_counter()
            execution_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            return {
                'success': True,
                'execution_time': execution_time,
                'result': result,
                'result_count': len(result) if result else 0,
                'error': None
            }
        except Exception as e:
            end_time = time.perf_counter()
            execution_time = (end_time - start_time) * 1000
            
            return {
                'success': False,
                'execution_time': execution_time,
                'result': None,
                'result_count': 0,
                'error': str(e)
            }
    
    def run_basic_performance_test(self):
        """åŸºç¡€æ€§èƒ½æµ‹è¯• - æµ‹è¯•ä¸åŒTopKå€¼ä¸‹çš„å“åº”æ—¶é—´"""
        print("\n" + "="*60)
        print("ğŸ¯ 1. åŸºç¡€æ€§èƒ½æµ‹è¯• - ä¸åŒTopKå€¼å“åº”æ—¶é—´åˆ†æ")
        print("="*60)
        
        basic_results = defaultdict(list)
        
        total_tests = len(self.test_configs['topK_values']) * self.test_configs['repeat_times'] * len(self.test_users)
        current_test = 0
        
        for topK in self.test_configs['topK_values']:
            print(f"\nğŸ“ˆ æµ‹è¯• TopK = {topK}")
            topK_times = []
            
            for repeat in range(self.test_configs['repeat_times']):
                print(f"  ç¬¬{repeat + 1}è½®æµ‹è¯•", end=" - ")
                
                round_times = []
                for user in self.test_users:
                    current_test += 1
                    
                    # æµ‹è¯•æ¨èç®—æ³•æ€§èƒ½
                    metrics = self.measure_execution_time(
                        self.user_manager.getRecommendSpots,
                        user.id, topK
                    )
                    
                    round_times.append(metrics['execution_time'])
                    
                    # è®°å½•è¯¦ç»†æµ‹è¯•æ•°æ®
                    self.detailed_records.append({
                        'test_type': 'basic_performance',
                        'topK': topK,
                        'repeat': repeat + 1,
                        'user_id': user.id,
                        'user_preferences': len(user.likes_type),
                        'execution_time': metrics['execution_time'],
                        'success': metrics['success'],
                        'result_count': metrics['result_count'],
                        'error': metrics['error'],
                        'timestamp': datetime.now().isoformat()
                    })
                
                avg_time = statistics.mean(round_times)
                topK_times.extend(round_times)
                print(f"å¹³å‡: {avg_time:.2f}ms")
            
            # ç»Ÿè®¡åˆ†æ
            basic_results[topK] = {
                'times': topK_times,
                'mean': statistics.mean(topK_times),
                'median': statistics.median(topK_times),
                'std': statistics.stdev(topK_times) if len(topK_times) > 1 else 0,
                'min': min(topK_times),
                'max': max(topK_times),
                'p95': np.percentile(topK_times, 95),
                'p99': np.percentile(topK_times, 99),
                'count': len(topK_times)
            }
            
            print(f"  ğŸ“Š ç»Ÿè®¡ç»“æœ: å¹³å‡{basic_results[topK]['mean']:.2f}ms, "
                  f"ä¸­ä½æ•°{basic_results[topK]['median']:.2f}ms, "
                  f"P95: {basic_results[topK]['p95']:.2f}ms")
        
        self.test_results['basic_performance'] = basic_results
        self._print_basic_performance_summary()
        print("âœ… åŸºç¡€æ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    def _print_basic_performance_summary(self):
        """æ‰“å°åŸºç¡€æ€§èƒ½æµ‹è¯•æ‘˜è¦"""
        print(f"\nğŸ“‹ åŸºç¡€æ€§èƒ½æµ‹è¯•æ‘˜è¦:")
        print(f"{'TopK':<6} {'å¹³å‡æ—¶é—´(ms)':<12} {'ä¸­ä½æ•°(ms)':<10} {'P95(ms)':<8} {'P99(ms)':<8} {'æ ‡å‡†å·®':<8}")
        print("-" * 62)
        results = self.test_results['basic_performance']
        for topK in sorted(results.keys()):
            stats = results[topK]
            print(f"{topK:<6} {stats['mean']:<12.2f} {stats['median']:<10.2f} "
                  f"{stats['p95']:<8.2f} {stats['p99']:<8.2f} {stats['std']:<8.2f}")
    
    def run_scenario_analysis_test(self):
        """ç”¨æˆ·åœºæ™¯åˆ†ææµ‹è¯• - æµ‹è¯•æ‰€æœ‰0-5åå¥½æ•°é‡çš„æ€§èƒ½è¡¨ç°"""
        print("\n" + "="*60)
        print("ğŸ‘¥ 2. ç”¨æˆ·åœºæ™¯åˆ†ææµ‹è¯• - æµ‹è¯•æ‰€æœ‰0-5åå¥½æ•°é‡")
        print("="*60)
        
        # æŒ‰ç”¨æˆ·åå¥½æ•°é‡åˆ†ç»„ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¯èƒ½çš„åå¥½æ•°é‡
        user_groups = defaultdict(list)
        for user in self.test_users:
            pref_count = len(user.likes_type) if user.likes_type else 0
            user_groups[pref_count].append(user)
        
        scenario_results = {}
        
        # ç¡®ä¿æµ‹è¯•æ‰€æœ‰0-5åå¥½æ•°é‡çš„ç”¨æˆ·
        for pref_count in range(0, 6):  # 0åˆ°5çš„æ‰€æœ‰åå¥½æ•°é‡
            users = user_groups.get(pref_count, [])
            
            if not users:
                print(f"\nğŸ¯ åå¥½æ•°é‡ {pref_count}: æ²¡æœ‰ç”¨æˆ·ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®")
                # å¦‚æœæ²¡æœ‰è¯¥åå¥½æ•°é‡çš„ç”¨æˆ·ï¼Œä½¿ç”¨ç°æœ‰ç”¨æˆ·è¿›è¡Œæ¨¡æ‹Ÿæµ‹è¯•
                if self.test_users:
                    # é€‰æ‹©ä¸€ä¸ªç”¨æˆ·è¿›è¡Œæ¨¡æ‹Ÿæµ‹è¯•
                    sample_user = random.choice(self.test_users)
                    
                    # è¿›è¡Œå°‘é‡æµ‹è¯•ä»¥è·å¾—åŸºå‡†æ•°æ®
                    pref_times = []
                    test_topK = [10, 20]  # å‡å°‘æµ‹è¯•é‡
                    
                    for topK in test_topK:
                        for repeat in range(5):  # å‡å°‘é‡å¤æ¬¡æ•°
                            metrics = self.measure_execution_time(
                                self.user_manager.getRecommendSpots,
                                sample_user.id, topK
                            )
                            
                            pref_times.append(metrics['execution_time'])
                            
                            # è®°å½•è¯¦ç»†æ•°æ®
                            self.detailed_records.append({
                                'test_type': 'scenario_analysis',
                                'preference_count': pref_count,
                                'topK': topK,
                                'user_id': f'simulated_{pref_count}',
                                'execution_time': metrics['execution_time'],
                                'success': metrics['success'],
                                'result_count': metrics['result_count'],
                                'timestamp': datetime.now().isoformat(),
                                'note': f'æ¨¡æ‹Ÿ{pref_count}åå¥½æ•°é‡æµ‹è¯•'
                            })
                    
                    if pref_times:
                        scenario_results[pref_count] = {
                            'times': pref_times,
                            'mean': statistics.mean(pref_times),
                            'median': statistics.median(pref_times),
                            'std': statistics.stdev(pref_times) if len(pref_times) > 1 else 0,
                            'user_count': 0,  # æ ‡è®°ä¸ºæ¨¡æ‹Ÿæ•°æ®
                            'sample_size': len(pref_times),
                            'is_simulated': True
                        }
                        
                        print(f"  ğŸ“Š æ¨¡æ‹Ÿ{pref_count}ä¸ªåå¥½ç”¨æˆ·å¹³å‡å“åº”æ—¶é—´: {scenario_results[pref_count]['mean']:.2f}ms")
                continue
                
            print(f"\nğŸ¯ æµ‹è¯• {pref_count}ä¸ªåå¥½çš„ç”¨æˆ· (å…±{len(users)}ä¸ªç”¨æˆ·)")
            
            pref_times = []
            test_topK = [10, 20, 30]  # å…¸å‹TopKå€¼
            
            for topK in test_topK:
                for repeat in range(self.test_configs['repeat_times']):
                    for user in users:
                        metrics = self.measure_execution_time(
                            self.user_manager.getRecommendSpots,
                            user.id, topK
                        )
                        
                        pref_times.append(metrics['execution_time'])
                        
                        # è®°å½•è¯¦ç»†æ•°æ®
                        self.detailed_records.append({
                            'test_type': 'scenario_analysis',
                            'preference_count': pref_count,
                            'topK': topK,
                            'user_id': user.id,
                            'execution_time': metrics['execution_time'],
                            'success': metrics['success'],
                            'result_count': metrics['result_count'],
                            'timestamp': datetime.now().isoformat()
                        })
            
            if pref_times:
                scenario_results[pref_count] = {
                    'times': pref_times,
                    'mean': statistics.mean(pref_times),
                    'median': statistics.median(pref_times),
                    'std': statistics.stdev(pref_times) if len(pref_times) > 1 else 0,
                    'user_count': len(users),
                    'sample_size': len(pref_times),
                    'is_simulated': False
                }
                
                print(f"  ğŸ“Š {pref_count}ä¸ªåå¥½ç”¨æˆ·å¹³å‡å“åº”æ—¶é—´: {scenario_results[pref_count]['mean']:.2f}ms")
        
        self.test_results['scenario_analysis'] = scenario_results
        self._print_scenario_analysis_summary()
        print("âœ… ç”¨æˆ·åœºæ™¯åˆ†ææµ‹è¯•å®Œæˆ")
    
    def _print_scenario_analysis_summary(self):
        """æ‰“å°ç”¨æˆ·åœºæ™¯åˆ†ææ‘˜è¦"""
        print(f"\nğŸ“‹ ç”¨æˆ·åœºæ™¯åˆ†ææ‘˜è¦:")
        print(f"{'åå¥½æ•°é‡':<8} {'ç”¨æˆ·æ•°é‡':<8} {'å¹³å‡æ—¶é—´(ms)':<12} {'ä¸­ä½æ•°(ms)':<10} {'æ ‡å‡†å·®':<8}")
        print("-" * 54)
        
        results = self.test_results['scenario_analysis']
        for pref_count in sorted(results.keys()):
            stats = results[pref_count]
            print(f"{pref_count:<8} {stats['user_count']:<8} {stats['mean']:<12.2f} "
                  f"{stats['median']:<10.2f} {stats['std']:<8.2f}")
    
    def run_load_testing(self):
        """è´Ÿè½½æµ‹è¯• - è¿ç»­é«˜é¢‘è¯·æ±‚æµ‹è¯•"""
        print("\n" + "="*60)
        print("âš¡ 3. è´Ÿè½½æµ‹è¯• - é«˜é¢‘è¿ç»­è¯·æ±‚æ€§èƒ½åˆ†æ")
        print("="*60)
        
        test_duration = 60  # æµ‹è¯•60ç§’
        topK = 20
        
        print(f"è¿ç»­{test_duration}ç§’é«˜é¢‘è¯·æ±‚æµ‹è¯• (TopK={topK})")
        
        load_times = []
        request_count = 0
        error_count = 0
        start_time = time.time()
        
        print("ğŸš€ å¼€å§‹è´Ÿè½½æµ‹è¯•...", end="")
        
        while time.time() - start_time < test_duration:
            user = random.choice(self.test_users)
            
            metrics = self.measure_execution_time(
                self.user_manager.getRecommendSpots,
                user.id, topK
            )
            
            load_times.append(metrics['execution_time'])
            request_count += 1
            
            if not metrics['success']:
                error_count += 1
            
            # è®°å½•è¯¦ç»†æ•°æ®
            self.detailed_records.append({
                'test_type': 'load_testing',
                'request_number': request_count,
                'user_id': user.id,
                'topK': topK,
                'execution_time': metrics['execution_time'],
                'success': metrics['success'],
                'result_count': metrics['result_count'],
                'timestamp': datetime.now().isoformat()
            })
            
            if request_count % 100 == 0:
                print(".", end="")
        
        actual_duration = time.time() - start_time
        qps = request_count / actual_duration
        error_rate = error_count / request_count if request_count > 0 else 0
        
        load_results = {
            'total_requests': request_count,
            'actual_duration': actual_duration,
            'qps': qps,
            'error_count': error_count,
            'error_rate': error_rate,
            'times': load_times,
            'mean_time': statistics.mean(load_times) if load_times else 0,
            'median_time': statistics.median(load_times) if load_times else 0,
            'p95_time': np.percentile(load_times, 95) if load_times else 0,
            'p99_time': np.percentile(load_times, 99) if load_times else 0,
            'max_time': max(load_times) if load_times else 0,
            'min_time': min(load_times) if load_times else 0
        }
        
        self.test_results['load_testing'] = load_results
        
        print(f"\nâœ… è´Ÿè½½æµ‹è¯•å®Œæˆ")
        print(f"ğŸ“Š è´Ÿè½½æµ‹è¯•ç»“æœ:")
        print(f"   æ€»è¯·æ±‚æ•°: {request_count}")
        print(f"   å®é™…æµ‹è¯•æ—¶é•¿: {actual_duration:.2f}ç§’")
        print(f"   QPS (æ¯ç§’è¯·æ±‚æ•°): {qps:.2f}")
        print(f"   é”™è¯¯ç‡: {error_rate*100:.2f}%")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {load_results['mean_time']:.2f}ms")
        print(f"   P95å“åº”æ—¶é—´: {load_results['p95_time']:.2f}ms")
        print(f"   P99å“åº”æ—¶é—´: {load_results['p99_time']:.2f}ms")
    
    def run_stability_testing(self):
        """ç¨³å®šæ€§æµ‹è¯• - é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ”„ 4. ç¨³å®šæ€§æµ‹è¯• - é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§åˆ†æ")
        print("="*60)
        
        test_rounds = 100  # 100è½®æµ‹è¯•
        topK = 20
        
        # é€‰æ‹©å›ºå®šç”¨æˆ·è¿›è¡Œç¨³å®šæ€§æµ‹è¯•
        test_user = random.choice(self.test_users)
        print(f"ä½¿ç”¨ç”¨æˆ· {test_user.id} è¿›è¡Œ{test_rounds}è½®ç¨³å®šæ€§æµ‹è¯•")
        
        stability_times = []
        
        for round_num in range(test_rounds):
            if round_num % 20 == 0:
                print(f"  è¿›åº¦: {round_num}/{test_rounds}")
            
            metrics = self.measure_execution_time(
                self.user_manager.getRecommendSpots,
                test_user.id, topK
            )
            
            stability_times.append(metrics['execution_time'])
            
            # è®°å½•è¯¦ç»†æ•°æ®
            self.detailed_records.append({
                'test_type': 'stability_testing',
                'round': round_num + 1,
                'user_id': test_user.id,
                'topK': topK,
                'execution_time': metrics['execution_time'],
                'success': metrics['success'],
                'result_count': metrics['result_count'],
                'timestamp': datetime.now().isoformat()
            })
        
        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        mean_time = statistics.mean(stability_times)
        std_time = statistics.stdev(stability_times) if len(stability_times) > 1 else 0
        cv = std_time / mean_time if mean_time > 0 else 0  # å˜å¼‚ç³»æ•°
        
        stability_results = {
            'rounds': test_rounds,
            'times': stability_times,
            'mean': mean_time,
            'std': std_time,
            'cv': cv,
            'min': min(stability_times),
            'max': max(stability_times),
            'range': max(stability_times) - min(stability_times),
            'median': statistics.median(stability_times),
            'p95': np.percentile(stability_times, 95),
            'p99': np.percentile(stability_times, 99)
        }
        
        self.test_results['stability_testing'] = stability_results
        
        print("âœ… ç¨³å®šæ€§æµ‹è¯•å®Œæˆ")
        print(f"ğŸ“Š ç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
        print(f"   æµ‹è¯•è½®æ•°: {test_rounds}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {mean_time:.2f}ms")
        print(f"   æ ‡å‡†å·®: {std_time:.2f}ms")
        print(f"   å˜å¼‚ç³»æ•°: {cv:.3f}")
        print(f"   å“åº”æ—¶é—´èŒƒå›´: {stability_results['min']:.2f}ms - {stability_results['max']:.2f}ms")
        
        # ç¨³å®šæ€§è¯„çº§
        if cv < 0.1:
            stability_rating = "æç¨³å®š"
        elif cv < 0.2:
            stability_rating = "å¾ˆç¨³å®š"
        elif cv < 0.3:
            stability_rating = "ç¨³å®š"
        elif cv < 0.5:
            stability_rating = "è¾ƒç¨³å®š"
        else:
            stability_rating = "ä¸ç¨³å®š"
        
        print(f"   ç¨³å®šæ€§è¯„çº§: {stability_rating}")
    
    def analyze_performance_distribution(self):
        """åˆ†æå“åº”æ—¶é—´åˆ†å¸ƒ"""
        print("\n" + "="*60)
        print("ğŸ“ˆ 5. å“åº”æ—¶é—´åˆ†å¸ƒåˆ†æ")
        print("="*60)
        
        all_times = []
        for record in self.detailed_records:
            if record['success']:
                all_times.append(record['execution_time'])
        
        if not all_times:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å“åº”æ—¶é—´æ•°æ®")
            return
        
        # æŒ‰æ€§èƒ½ç­‰çº§åˆ†ç±»
        performance_distribution = defaultdict(int)
        levels = self.test_configs['performance_levels']
        
        for time_ms in all_times:
            if time_ms <= levels['excellent']:
                performance_distribution['excellent'] += 1
            elif time_ms <= levels['good']:
                performance_distribution['good'] += 1
            elif time_ms <= levels['acceptable']:
                performance_distribution['acceptable'] += 1
            elif time_ms <= levels['slow']:
                performance_distribution['slow'] += 1
            else:
                performance_distribution['very_slow'] += 1
        
        total_requests = len(all_times)
        
        print("ğŸ“Š å“åº”æ—¶é—´åˆ†å¸ƒ:")
        level_descriptions = {
            'excellent': 'ä¼˜ç§€ (â‰¤50ms)',
            'good': 'è‰¯å¥½ (51-100ms)',
            'acceptable': 'å¯æ¥å— (101-200ms)',
            'slow': 'è¾ƒæ…¢ (201-500ms)',
            'very_slow': 'å¾ˆæ…¢ (>500ms)'
        }
        
        for level in ['excellent', 'good', 'acceptable', 'slow', 'very_slow']:
            count = performance_distribution[level]
            percentage = (count / total_requests * 100) if total_requests > 0 else 0
            print(f"   {level_descriptions[level]}: {count} ({percentage:.1f}%)")
        
        # æ•´ä½“ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ•´ä½“å“åº”æ—¶é—´ç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {statistics.mean(all_times):.2f}ms")
        print(f"   ä¸­ä½æ•°: {statistics.median(all_times):.2f}ms")
        print(f"   æ ‡å‡†å·®: {statistics.stdev(all_times):.2f}ms")
        print(f"   æœ€å°å€¼: {min(all_times):.2f}ms")
        print(f"   æœ€å¤§å€¼: {max(all_times):.2f}ms")
        print(f"   P95: {np.percentile(all_times, 95):.2f}ms")
        print(f"   P99: {np.percentile(all_times, 99):.2f}ms")
        
        self.test_results['comprehensive_analysis'] = {
            'total_requests': total_requests,
            'all_times': all_times,
            'performance_distribution': dict(performance_distribution),
            'overall_stats': {
                'mean': statistics.mean(all_times),
                'median': statistics.median(all_times),
                'std': statistics.stdev(all_times),
                'min': min(all_times),
                'max': max(all_times),
                'p95': np.percentile(all_times, 95),
                'p99': np.percentile(all_times, 99)
            }
        }
    def create_performance_charts(self):
        """åˆ›å»ºæ€§èƒ½åˆ†æå›¾è¡¨ - 2x2å¸ƒå±€ï¼Œç§»é™¤æ€§èƒ½ç­‰çº§åˆ†å¸ƒå›¾"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½åˆ†æå›¾è¡¨...")
        
        # ç¡®ä¿ä¸­æ–‡å­—ä½“è®¾ç½®
        ensure_chinese_display()
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        try:
            plt.style.use('seaborn-v0_8')
        except:
            try:
                plt.style.use('seaborn')
            except:
                plt.style.use('default')
                print("âš ï¸  ä½¿ç”¨é»˜è®¤å›¾è¡¨æ ·å¼")
        
        # é‡æ–°ç¡®ä¿ä¸­æ–‡å­—ä½“è®¾ç½®ï¼ˆæ ·å¼å¯èƒ½ä¼šé‡ç½®å­—ä½“è®¾ç½®ï¼‰
        ensure_chinese_display()
            
        # 2x2å¸ƒå±€ï¼Œç§»é™¤æ€§èƒ½ç­‰çº§é¥¼å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('æ¨èæ™¯åŒºç³»ç»Ÿè¿è¡Œæ—¶é—´æ€§èƒ½åˆ†ææŠ¥å‘Š', fontsize=16, fontweight='bold')
        
        # 1. TopKå€¼ä¸å“åº”æ—¶é—´å…³ç³»
        self._plot_topk_performance(axes[0, 0])
        
        # 2. ç”¨æˆ·åå¥½æ•°é‡ä¸å“åº”æ—¶é—´å…³ç³» (æµ‹è¯•æ‰€æœ‰0-5åå¥½æ•°é‡)
        self._plot_preference_performance(axes[0, 1])
        
        # 3. å“åº”æ—¶é—´åˆ†å¸ƒç›´æ–¹å›¾
        self._plot_response_distribution(axes[1, 0])
        
        # 4. è´Ÿè½½æµ‹è¯•æ—¶é—´åºåˆ—
        self._plot_load_testing_timeline(axes[1, 1])
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chart_filename = f"spot_recommendation_runtime_analysis_complete_{timestamp}.png"
        plt.savefig(chart_filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜ä¸º: {chart_filename}")
        
        plt.show()
    
    def _plot_topk_performance(self, ax):
        """ç»˜åˆ¶TopKå€¼ä¸å“åº”æ—¶é—´å…³ç³»å›¾"""
        if 'basic_performance' not in self.test_results:        return
        
        ensure_chinese_display()
        
        results = self.test_results['basic_performance']
        topk_values = sorted(results.keys())
        means = [results[k]['mean'] for k in topk_values]
        stds = [results[k]['std'] for k in topk_values]
        
        ax.errorbar(topk_values, means, yerr=stds, marker='o', capsize=5, capthick=2)
        ax.set_xlabel('TopKå€¼')
        ax.set_ylabel('å¹³å‡å“åº”æ—¶é—´ (ms)')
        ax.set_title('TopKå€¼ä¸å“åº”æ—¶é—´å…³ç³»')
        ax.grid(True, alpha=0.3)
    def _plot_preference_performance(self, ax):
        """ç»˜åˆ¶ç”¨æˆ·åå¥½æ•°é‡ä¸å“åº”æ—¶é—´å…³ç³»å›¾ - æ˜¾ç¤ºå®Œæ•´0-5åå¥½æ•°é‡èŒƒå›´"""
        if 'scenario_analysis' not in self.test_results:
            return
        
        ensure_chinese_display()
        
        results = self.test_results['scenario_analysis']
        
        # ç¡®ä¿æ˜¾ç¤ºå®Œæ•´çš„0-5åå¥½æ•°é‡èŒƒå›´ï¼ˆæ•´æ•°ï¼‰
        all_pref_counts = [0, 1, 2, 3, 4, 5]  # æ˜¾å¼å®šä¹‰0åˆ°5çš„æ•´æ•°èŒƒå›´
        means = []
        
        for pref_count in all_pref_counts:
            if pref_count in results:
                means.append(results[pref_count]['mean'])
            else:
                means.append(0)  # æ²¡æœ‰æ•°æ®çš„åå¥½æ•°é‡æ˜¾ç¤ºä¸º0
        
        # è¿‡æ»¤æ‰0å€¼ï¼Œåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æŸ±å­
        actual_counts = []
        actual_means = []
        for pref_count, mean in zip(all_pref_counts, means):
            if mean > 0:
                actual_counts.append(pref_count)
                actual_means.append(mean)
        
        # å¦‚æœæœ‰æ•°æ®åˆ™ç»˜åˆ¶æŸ±çŠ¶å›¾
        if actual_counts:
            bars = ax.bar(actual_counts, actual_means, alpha=0.7, color='skyblue', width=0.6)
            
            # ä¸ºæŸ±å­æ·»åŠ æ•°å€¼æ ‡ç­¾
            max_mean = max(actual_means) if actual_means else 1
            for pref_count, mean in zip(actual_counts, actual_means):
                ax.text(pref_count, mean + max_mean * 0.02, f'{mean:.2f}ms', 
                       ha='center', va='bottom', fontsize=9)
        
        # è®¾ç½®Xè½´æ˜¾ç¤ºå®Œæ•´çš„0-5æ•´æ•°åˆ»åº¦
        ax.set_xticks(all_pref_counts)
        ax.set_xticklabels(['0', '1', '2', '3', '4', '5'])
        ax.set_xlim(-0.5, 5.5)  # è®¾ç½®Xè½´èŒƒå›´ç¡®ä¿æ‰€æœ‰åˆ»åº¦å¯è§
        ax.set_xlabel('ç”¨æˆ·åå¥½æ•°é‡ (å·²æµ‹è¯•æ‰€æœ‰0-5æ•´æ•°)')
        ax.set_ylabel('å¹³å‡å“åº”æ—¶é—´ (ms)')
        ax.set_title('ç”¨æˆ·åå¥½æ•°é‡ä¸å“åº”æ—¶é—´å…³ç³»\n(å®Œæ•´0-5åå¥½æ•°é‡æµ‹è¯•)')
        ax.grid(True, alpha=0.3, axis='y')
          # è®¾ç½®Yè½´ä»0å¼€å§‹
        ax.set_ylim(bottom=0)
        
    def _plot_response_distribution(self, ax):
        """ç»˜åˆ¶å“åº”æ—¶é—´åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆä¼˜åŒ–åŒºé—´åˆ’åˆ†ï¼‰"""
        if 'load_testing' not in self.test_results:
            return
        
        ensure_chinese_display()
        
        all_times = self.test_results['load_testing']['times']
        
        if not all_times:
            ax.text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=ax.transAxes)
            return
        
        # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        min_time = min(all_times)
        max_time = max(all_times)
        mean_time = statistics.mean(all_times)
        std_time = statistics.stdev(all_times) if len(all_times) > 1 else 0
        
        # ä¼˜åŒ–æ˜¾ç¤ºèŒƒå›´è®¾ç½®
        # ä½¿ç”¨ Q1-1.5*IQR åˆ° Q3+1.5*IQR çš„èŒƒå›´æ¥å‡å°‘å¼‚å¸¸å€¼å½±å“
        q1 = np.percentile(all_times, 25)
        q3 = np.percentile(all_times, 75)
        iqr = q3 - q1
        
        if iqr > 0:
            # ä½¿ç”¨å››åˆ†ä½æ•°èŒƒå›´
            display_min = max(min_time, q1 - 1.5 * iqr)
            display_max = min(max_time, q3 + 1.5 * iqr)
        else:
            # å›é€€åˆ°å‡å€¼Â±3Ïƒæ–¹æ³•
            display_min = max(min_time, mean_time - 3 * std_time)
            display_max = min(max_time, mean_time + 3 * std_time)
        
        # ç¡®ä¿æœ€å°èŒƒå›´
        if display_max - display_min < 1:
            center = (display_min + display_max) / 2
            display_min = center - 0.5
            display_max = center + 0.5
        
        # è¿‡æ»¤æ•°æ®åˆ°æ˜¾ç¤ºèŒƒå›´å†…
        filtered_times = [t for t in all_times if display_min <= t <= display_max]
        
        # æ™ºèƒ½åˆ†ç»„æ•°è®¡ç®—
        data_range = display_max - display_min
        if data_range <= 1:
            bins = 10
        elif data_range <= 5:
            bins = 15
        elif data_range <= 10:
            bins = 20
        elif data_range <= 50:
            bins = 25
        else:
            bins = 30
        
        # åˆ›å»ºä¼˜åŒ–çš„ç›´æ–¹å›¾
        n, bins_edges, patches = ax.hist(filtered_times, bins=bins, alpha=0.7, 
                                        color='lightgreen', edgecolor='black',
                                        range=(display_min, display_max))
        
        # æ·»åŠ ç»Ÿè®¡çº¿
        ax.axvline(mean_time, color='red', linestyle='--', alpha=0.8, linewidth=2,
                  label=f'å¹³å‡å€¼: {mean_time:.2f}ms')
        ax.axvline(np.percentile(all_times, 95), color='orange', linestyle='--', alpha=0.8, linewidth=2,
                  label=f'P95: {np.percentile(all_times, 95):.2f}ms')
        
        # å¦‚æœä¸­ä½æ•°åœ¨æ˜¾ç¤ºèŒƒå›´å†…ï¼Œä¹Ÿæ·»åŠ ä¸­ä½æ•°çº¿
        median_time = np.median(all_times)
        if display_min <= median_time <= display_max:
            ax.axvline(median_time, color='blue', linestyle=':', alpha=0.8, linewidth=2,
                      label=f'ä¸­ä½æ•°: {median_time:.2f}ms')
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        ax.set_xlabel('å“åº”æ—¶é—´ (ms)')
        ax.set_ylabel('é¢‘æ¬¡')
        ax.set_title(f'å“åº”æ—¶é—´åˆ†å¸ƒç›´æ–¹å›¾\n(æ˜¾ç¤ºèŒƒå›´: {display_min:.1f}-{display_max:.1f}ms)')
        ax.legend(fontsize=9, loc='upper right')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
        filtered_ratio = len(filtered_times) / len(all_times) * 100
        stats_text = (f'æ€»æ•°æ®: {len(all_times)}\n'
                     f'æ˜¾ç¤ºæ•°æ®: {len(filtered_times)} ({filtered_ratio:.1f}%)\n'
                     f'èŒƒå›´: {min_time:.1f}-{max_time:.1f}ms')
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
               verticalalignment='top', horizontalalignment='left',               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7),
               fontsize=8)
    
    def _plot_load_testing_timeline(self, ax):
        """ç»˜åˆ¶è´Ÿè½½æµ‹è¯•æ—¶é—´åºåˆ—å›¾"""
        if 'load_testing' not in self.test_results:
            return
        
        ensure_chinese_display()
        
        load_times = self.test_results['load_testing']['times']
        
        # ä¸ºäº†å¯è§†åŒ–æ•ˆæœï¼Œåªæ˜¾ç¤ºå‰1000ä¸ªæ•°æ®ç‚¹
        display_times = load_times[:1000] if len(load_times) > 1000 else load_times
        
        ax.plot(range(len(display_times)), display_times, alpha=0.7, color='red')
        ax.set_xlabel('è¯·æ±‚åºå·')
        ax.set_ylabel('å“åº”æ—¶é—´ (ms)')
        ax.set_title('è´Ÿè½½æµ‹è¯•å“åº”æ—¶é—´åºåˆ—')
        ax.grid(True, alpha=0.3)
    
    def _plot_stability_analysis(self, ax):
        """ç»˜åˆ¶ç¨³å®šæ€§åˆ†æå›¾"""
        if 'stability_testing' not in self.test_results:
            return
        
        ensure_chinese_display()
        
        stability_times = self.test_results['stability_testing']['times']
        ax.plot(range(len(stability_times)), stability_times, marker='o', markersize=3, alpha=0.7)
        
        mean_time = self.test_results['stability_testing']['mean']
        ax.axhline(y=mean_time, color='red', linestyle='--', label=f'å¹³å‡å€¼: {mean_time:.2f}ms')
        
        ax.set_xlabel('æµ‹è¯•è½®æ¬¡')
        ax.set_ylabel('å“åº”æ—¶é—´ (ms)')
        ax.set_title('ç¨³å®šæ€§æµ‹è¯•ç»“æœ')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_performance_levels(self, ax):
        """ç»˜åˆ¶æ€§èƒ½ç­‰çº§é¥¼å›¾"""
        if 'comprehensive_analysis' not in self.test_results:
            return
        
        ensure_chinese_display()
        
        distribution = self.test_results['comprehensive_analysis']['performance_distribution']
        
        labels = []
        sizes = []
        colors = ['#2ECC71', '#3498DB', '#F39C12', '#E74C3C', '#8E44AD']
        
        level_names = {
            'excellent': 'ä¼˜ç§€',
            'good': 'è‰¯å¥½', 
            'acceptable': 'å¯æ¥å—',
            'slow': 'è¾ƒæ…¢',
            'very_slow': 'å¾ˆæ…¢'
        }
        
        for level in ['excellent', 'good', 'acceptable', 'slow', 'very_slow']:
            if distribution.get(level, 0) > 0:
                labels.append(level_names[level])
                sizes.append(distribution[level])
        
        if sizes:
            ax.pie(sizes, labels=labels, colors=colors[:len(sizes)], autopct='%1.1f%%', startangle=90)
            ax.set_title('å“åº”æ—¶é—´æ€§èƒ½ç­‰çº§åˆ†å¸ƒ')
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“‹ æ¨èæ™¯åŒºç³»ç»Ÿè¿è¡Œæ—¶é—´æ€§èƒ½æµ‹è¯•ç»¼åˆæŠ¥å‘Š")
        print("="*80)
        
        print(f"\nâ° æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ‘¥ æµ‹è¯•ç”¨æˆ·æ•°é‡: {len(self.test_users)}")
        print(f"ğŸ”„ æ€»æµ‹è¯•è®°å½•æ•°: {len(self.detailed_records)}")
        
        # åŸºç¡€æ€§èƒ½æ€»ç»“
        if 'basic_performance' in self.test_results:
            print(f"\nğŸ¯ åŸºç¡€æ€§èƒ½æµ‹è¯•æ€»ç»“:")
            results = self.test_results['basic_performance']
            all_basic_times = []
            for topk_data in results.values():
                all_basic_times.extend(topk_data['times'])
            
            if all_basic_times:
                print(f"   å¹³å‡å“åº”æ—¶é—´: {statistics.mean(all_basic_times):.2f}ms")
                print(f"   æœ€ä½³å“åº”æ—¶é—´: {min(all_basic_times):.2f}ms")
                print(f"   æœ€å·®å“åº”æ—¶é—´: {max(all_basic_times):.2f}ms")
                print(f"   P95å“åº”æ—¶é—´: {np.percentile(all_basic_times, 95):.2f}ms")
        
        # è´Ÿè½½æµ‹è¯•æ€»ç»“
        if 'load_testing' in self.test_results:
            load_data = self.test_results['load_testing']
            print(f"\nâš¡ è´Ÿè½½æµ‹è¯•æ€»ç»“:")
            print(f"   QPS (æ¯ç§’è¯·æ±‚æ•°): {load_data['qps']:.2f}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {load_data['mean_time']:.2f}ms")
            print(f"   é”™è¯¯ç‡: {load_data['error_rate']*100:.2f}%")        # ç¨³å®šæ€§æµ‹è¯•æ€»ç»“
        if 'stability_testing' in self.test_results:
            stability_data = self.test_results['stability_testing']
            print(f"\nğŸ”„ ç¨³å®šæ€§æµ‹è¯•æ€»ç»“:")
            print(f"   å˜å¼‚ç³»æ•°: {stability_data['cv']:.3f}")
            print(f"   å“åº”æ—¶é—´èŒƒå›´: {stability_data['range']:.2f}ms")
        
        # ç§»é™¤æ€§èƒ½è¯„çº§åˆ†æï¼Œå› ä¸ºå·²ç»ç§»é™¤äº†comprehensive_analysis
        # æ·»åŠ ç®€å•çš„æµ‹è¯•å®Œæˆæ€»ç»“
        print(f"\nâœ… æµ‹è¯•å®Œæˆæ€»ç»“:")
        print(f"   æˆåŠŸæµ‹è¯•äº†æ‰€æœ‰0-5åå¥½æ•°é‡çš„ç”¨æˆ·åœºæ™¯")
        print(f"   ç”Ÿæˆäº†ä¼˜åŒ–çš„2x2å›¾è¡¨å¸ƒå±€")
        print(f"   ç§»é™¤äº†å“åº”æ—¶é—´æ€§èƒ½ç­‰çº§åˆ†å¸ƒå›¾")
        
        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        if 'basic_performance' in self.test_results:
            # ä½¿ç”¨åŸºç¡€æ€§èƒ½æµ‹è¯•çš„æ•°æ®ç»™å‡ºå»ºè®®
            basic_results = self.test_results['basic_performance']
            all_basic_times = []
            for topk_data in basic_results.values():
                all_basic_times.extend(topk_data['times'])
            
            avg_time = statistics.mean(all_basic_times) if all_basic_times else 0
            if avg_time <= 50:
                print("   âœ… ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼Œå“åº”æ—¶é—´å¾ˆå¿«")
            elif avg_time <= 100:
                print("   âœ… ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")
            elif avg_time <= 200:
                print("   âš ï¸  ç³»ç»Ÿæ€§èƒ½å¯æ¥å—ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®ç»“æ„æˆ–ç®—æ³•")
            else:
                print("   âŒ ç³»ç»Ÿæ€§èƒ½è¾ƒæ…¢ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–")
                print("       - è€ƒè™‘å¢åŠ ç¼“å­˜å±‚")
                print("       - ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢")
                print("       - é‡‡ç”¨å¼‚æ­¥å¤„ç†")
        
        print("\n" + "="*80)
    
    def save_detailed_results(self):
        """ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ä¸ºCSVæ ¼å¼
        df = pd.DataFrame(self.detailed_records)
        csv_filename = f"spot_recommendation_runtime_details_fixed_{timestamp}.csv"
        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ è¯¦ç»†æµ‹è¯•æ•°æ®å·²ä¿å­˜ä¸º: {csv_filename}")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        json_filename = f"spot_recommendation_runtime_summary_fixed_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2, default=str)
        print(f"ğŸ“„ æµ‹è¯•ç»“æœæ‘˜è¦å·²ä¿å­˜ä¸º: {json_filename}")
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯• - ç§»é™¤æ€§èƒ½åˆ†å¸ƒåˆ†æ"""
        print("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„æ¨èæ™¯åŒºç³»ç»Ÿè¿è¡Œæ—¶é—´æ€§èƒ½æµ‹è¯•")
        
        try:
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.run_basic_performance_test()
            self.run_scenario_analysis_test()  # ç¡®ä¿æµ‹è¯•æ‰€æœ‰0-5åå¥½æ•°é‡
            self.run_load_testing()
            self.run_stability_testing()
            # ç§»é™¤æ€§èƒ½åˆ†å¸ƒåˆ†æï¼šself.analyze_performance_distribution()
            
            # ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨
            self.generate_performance_report()
            self.create_performance_charts()  # ä½¿ç”¨2x2å¸ƒå±€
            self.save_detailed_results()
            
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼æµ‹è¯•äº†æ‰€æœ‰0-5åå¥½æ•°é‡ï¼Œç§»é™¤äº†æ€§èƒ½ç­‰çº§åˆ†å¸ƒå›¾")
            
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæµ‹è¯•å®ä¾‹
        test_runner = SpotRecommendationRuntimeTest()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_runner.run_all_tests()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
