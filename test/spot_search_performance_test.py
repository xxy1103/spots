# -*- coding: utf-8 -*-
"""
æ™¯åŒºæœç´¢ç³»ç»Ÿæ€§èƒ½æµ‹è¯•ç¨‹åº
ä¸“é—¨æµ‹è¯•æ™¯åŒºæœç´¢åŠŸèƒ½çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬å…³é”®å­—æœç´¢ã€ç±»å‹ç­›é€‰ã€æ’åºç­‰æ ¸å¿ƒåŠŸèƒ½
æä¾›è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
"""

import sys
import os
import time
import random
import statistics
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import pandas as pd
import seaborn as sns
import json
import threading
import concurrent.futures
from datetime import datetime
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ æ¨¡å—è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

try:
    from module.Spot_class import SpotManager
    from module.fileIo import ConfigIo, spotIo
    from module.data_structure.hashtable import HashTable
    from module.data_structure.indexHeap import TopKHeap
    from module.data_structure.heap import MinHeap
    from module.data_structure.set import MySet
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    sys.exit(1)

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º"""
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['axes.unicode_minus'] = False
    
    try:
        fm._rebuild()
        print("âœ… å­—ä½“é…ç½®å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ å­—ä½“é…ç½®è­¦å‘Š: {e}")

class SpotSearchPerformanceTester:
    """æ™¯åŒºæœç´¢ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        print("ğŸš€ åˆå§‹åŒ–æ™¯åŒºæœç´¢æ€§èƒ½æµ‹è¯•å™¨...")
        
        # åŠ è½½æ™¯åŒºæ•°æ®
        print("  ğŸ“ åŠ è½½æ™¯åŒºæ•°æ®...")
        try:
            spots_data = spotIo.load_spots()
            print(f"  âœ… æˆåŠŸåŠ è½½ {spots_data.get('counts', 0)} ä¸ªæ™¯åŒº")
        except Exception as e:
            print(f"  âŒ åŠ è½½æ™¯åŒºæ•°æ®å¤±è´¥: {e}")
            raise e
        
        # åˆå§‹åŒ– SpotManager
        print("  ğŸ—ï¸  åˆå§‹åŒ–SpotManager...")
        try:
            self.spot_manager = SpotManager.from_dict(spots_data)
            print("  âœ… SpotManager åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ SpotManager åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e
        
        # è·å–æ‰€æœ‰æ™¯åŒºæ•°æ®
        self.all_spots = self.spot_manager.spots
        self.total_spots = len(self.all_spots)
        
        # è·å–æ‰€æœ‰æ™¯åŒºç±»å‹
        try:
            config_io = ConfigIo()
            self.all_types = config_io.getAllSpotTypes()
            print(f"  âœ… æˆåŠŸåŠ è½½ {len(self.all_types)} ç§æ™¯åŒºç±»å‹")
        except Exception as e:
            print(f"  âš ï¸ åŠ è½½æ™¯åŒºç±»å‹å¤±è´¥: {e}")
            self.all_types = ["å†å²å»ºç­‘", "èµèŠ±èƒœåœ°", "èŒèŒåŠ¨ç‰©", "åŸå¸‚æ¼«æ­¥", "å¤œæ¸¸è§‚æ™¯", 
                             "é›å¨ƒå®è—åœ°", "å±•é¦†å±•è§ˆ", "åœ°æ ‡è§‚æ™¯", "ç™»é«˜çˆ¬å±±", "è¸é’å¿…å»", 
                             "è‡ªç„¶å±±æ°´", "æ¸¸ä¹åœº", "æ¼”å‡º"]
        
        # æ€§èƒ½æ•°æ®æ”¶é›†
        self.performance_data = defaultdict(list)
        self.search_statistics = {}
          # æµ‹è¯•é…ç½®
        self.test_rounds = 100  # æ¯ä¸ªæµ‹è¯•çš„è½®æ¬¡
        self.concurrent_levels = [1, 5, 10, 20, 50]  # å¹¶å‘æµ‹è¯•çº§åˆ«
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ€»æ™¯åŒºæ•°é‡: {self.total_spots}")
        print(f"   æ™¯åŒºç±»å‹æ•°é‡: {len(self.all_types)}")
        print(f"   æ™¯åŒºç±»å‹: {', '.join(self.all_types)}")
        print(f"ğŸ“Š æ™¯åŒºæ€»æ•°: {self.total_spots}")
        print(f"ğŸ·ï¸ æ™¯åŒºç±»å‹æ•°: {len(self.all_types)}")
        print(f"ğŸ”„ æµ‹è¯•è½®æ¬¡: {self.test_rounds}")
        
    def measure_time(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        return result, (end_time - start_time) * 1000  # è¿”å›æ¯«ç§’
    
    def test_keyword_search_performance(self):
        """æµ‹è¯•å…³é”®å­—æœç´¢æ€§èƒ½"""
        print("\nğŸ” æµ‹è¯•å…³é”®å­—æœç´¢æ€§èƒ½...")
          # å‡†å¤‡ä¸åŒé•¿åº¦çš„æµ‹è¯•å…³é”®å­—
        keywords_by_length = {1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: []}
        
        # ä»çœŸå®æ™¯åŒºåç§°ä¸­æå–ä¸åŒé•¿åº¦çš„å…³é”®å­—
        for spot in self.all_spots[:50]:
            name = spot.name
            for length in [1, 2, 3, 4, 5, 6, 7, 8]:
                if len(name) >= length:
                    for i in range(len(name) - length + 1):
                        keyword = name[i:i+length]
                        if keyword not in keywords_by_length[length] and len(keywords_by_length[length]) < 5:
                            keywords_by_length[length].append(keyword)
        
        # æ·»åŠ ä¸€äº›å¸¸è§æœç´¢è¯
        keywords_by_length[1].extend(["å±±", "æ¹–", "å¤", "æ–°", "å¤§"])
        keywords_by_length[2].extend(["å…¬å›­", "å¯ºåº™", "åšç‰©", "åŒ—äº¬", "å¤©å®‰"])
        keywords_by_length[3].extend(["åšç‰©é¦†", "é¢å’Œå›­", "å¤©å›", "æ•…å®«"])
        keywords_by_length[4].extend(["å¤©å®‰é—¨å¹¿åœº", "é¢å’Œå›­"])
        keywords_by_length[5].extend(["æ•…å®«åšç‰©é™¢", "åŒ—äº¬åŠ¨ç‰©å›­"])
        keywords_by_length[6].extend(["ä¸­å›½å›½å®¶åšç‰©é¦†", "åŒ—äº¬è‡ªç„¶åšç‰©é¦†"])
        keywords_by_length[7].extend(["ä¸­å›½ç§‘å­¦æŠ€æœ¯é¦†", "åŒ—äº¬å¤©æ–‡é¦†"])
        keywords_by_length[8].extend(["ä¸­å›½äººæ°‘é©å‘½å†›äº‹åšç‰©é¦†"])
        
        # é™åˆ¶æ¯ç§é•¿åº¦çš„å…³é”®å­—æ•°é‡
        for length in keywords_by_length:
            keywords_by_length[length] = keywords_by_length[length][:5]
        
        search_times = []
        search_results_count = []
        search_times_by_length = {1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: []}
        
        for length, keywords in keywords_by_length.items():
            print(f"  æµ‹è¯•{length}å­—ç¬¦å…³é”®å­—...")
            for keyword in keywords:
                times_for_keyword = []
                for _ in range(10):  # æ¯ä¸ªå…³é”®å­—æµ‹è¯•10æ¬¡
                    result, exec_time = self.measure_time(
                        self.spot_manager.getSpotByName, keyword
                    )
                    times_for_keyword.append(exec_time)
                    
                avg_time = statistics.mean(times_for_keyword)
                search_times.append(avg_time)
                search_times_by_length[length].append(avg_time)
                search_results_count.append(len(result) if result else 0)
                
                print(f"    å…³é”®å­— '{keyword}': {avg_time:.3f}ms, ç»“æœæ•°: {len(result) if result else 0}")
          # ç»Ÿè®¡æ•°æ®
        self.performance_data['keyword_search_times'] = search_times
        self.performance_data['keyword_search_times_by_length'] = search_times_by_length
        self.performance_data['keyword_search_results'] = search_results_count
        
        self.search_statistics['keyword_search'] = {
            'avg_time': statistics.mean(search_times),
            'min_time': min(search_times),
            'max_time': max(search_times),
            'median_time': statistics.median(search_times),
            'std_time': statistics.stdev(search_times) if len(search_times) > 1 else 0,
            'avg_results': statistics.mean(search_results_count),
            'total_keywords_tested': sum(len(keywords) for keywords in keywords_by_length.values())
        }
        
        print(f"âœ… å…³é”®å­—æœç´¢æµ‹è¯•å®Œæˆ")
        print(f"   å¹³å‡æœç´¢æ—¶é—´: {self.search_statistics['keyword_search']['avg_time']:.3f}ms")
        print(f"   å¹³å‡ç»“æœæ•°é‡: {self.search_statistics['keyword_search']['avg_results']:.1f}")
    
    def test_type_based_search_performance(self):
        """æµ‹è¯•åŸºäºç±»å‹çš„æœç´¢æ€§èƒ½"""
        print("\nğŸ·ï¸ æµ‹è¯•ç±»å‹æœç´¢æ€§èƒ½...")
        
        type_search_times = []
        type_search_results = []
        
        for spot_type in self.all_types:
            times_for_type = []
            for k in [5, 10, 20, 50]:  # æµ‹è¯•ä¸åŒçš„kå€¼
                result, exec_time = self.measure_time(
                    self.spot_manager.getTopKByType, spot_type, k
                )
                times_for_type.append(exec_time)
            
            avg_time = statistics.mean(times_for_type)
            type_search_times.append(avg_time)
            
            # è·å–è¯¥ç±»å‹çš„æ€»æ™¯åŒºæ•°
            result, _ = self.measure_time(
                self.spot_manager.getTopKByType, spot_type, 1000
            )
            type_search_results.append(len(result) if result else 0)
            
            print(f"  ç±»å‹ '{spot_type}': {avg_time:.3f}ms, æ€»æ•°: {len(result) if result else 0}")
        
        self.performance_data['type_search_times'] = type_search_times
        self.performance_data['type_search_results'] = type_search_results
        self.search_statistics['type_search'] = {
            'avg_time': statistics.mean(type_search_times),
            'min_time': min(type_search_times),
            'max_time': max(type_search_times),
            'median_time': statistics.median(type_search_times),
            'std_time': statistics.stdev(type_search_times) if len(type_search_times) > 1 else 0,
            'avg_results': statistics.mean(type_search_results),
            'total_types_tested': len(self.all_types)
        }        
        print(f"âœ… ç±»å‹æœç´¢æµ‹è¯•å®Œæˆ")
        print(f"   å¹³å‡æœç´¢æ—¶é—´: {self.search_statistics['type_search']['avg_time']:.3f}ms")
    
    def test_sorting_performance(self):
        """æµ‹è¯•æ’åºæ€§èƒ½"""
        print("\nğŸ“Š æµ‹è¯•æ’åºæ€§èƒ½...")
        
        # æµ‹è¯•getAllSpotsSortedæ–¹æ³•çš„æ€§èƒ½
        times = []
        for _ in range(20):  # æµ‹è¯•20æ¬¡
            result, exec_time = self.measure_time(
                self.spot_manager.getAllSpotsSorted
            )
            times.append(exec_time)
        
        avg_time = statistics.mean(times)
        sorting_times = {
            'getAllSpotsSorted': {
                'avg_time': avg_time,
                'min_time': min(times),
                'max_time': max(times),
                'std_time': statistics.stdev(times) if len(times) > 1 else 0
            }
        }
        
        print(f"  å…¨æ’åºæ€§èƒ½: {avg_time:.3f}ms")
        
        self.performance_data['sorting_times'] = sorting_times
        self.search_statistics['sorting'] = sorting_times
        
        print(f"âœ… æ’åºæ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    def test_data_structure_performance(self):
        """æµ‹è¯•æ•°æ®ç»“æ„æ€§èƒ½"""
        print("\nğŸ”§ æµ‹è¯•æ•°æ®ç»“æ„æ€§èƒ½...")
        
        # æµ‹è¯•å“ˆå¸Œè¡¨æ€§èƒ½
        print("  æµ‹è¯•å“ˆå¸Œè¡¨æ€§èƒ½...")
        hashtable = HashTable()
          # æ’å…¥æµ‹è¯•
        insert_times = []
        for i in range(1000):
            test_item = {"id": i, "name": f"æµ‹è¯•æ™¯ç‚¹{i}"}
            _, exec_time = self.measure_time(hashtable.insert, test_item)
            insert_times.append(exec_time)
        
        # æŸ¥æ‰¾æµ‹è¯•
        search_times = []
        for i in range(100):
            search_char = "æµ‹"  # æœç´¢åŒ…å«"æµ‹"å­—çš„é¡¹ç›®
            _, exec_time = self.measure_time(hashtable.search, search_char)
            search_times.append(exec_time)
        
        hashtable_stats = {
            'avg_insert_time': statistics.mean(insert_times),
            'avg_search_time': statistics.mean(search_times),
            'total_operations': len(insert_times) + len(search_times)
        }
          # æµ‹è¯•å †æ€§èƒ½
        print("  æµ‹è¯•å †æ€§èƒ½...")
        top_k_heap = TopKHeap()
        min_heap = MinHeap()
        
        heap_insert_times = []
        test_data = [(random.random() * 100, f"item_{i}") for i in range(500)]
        
        for score, item in test_data:
            _, exec_time = self.measure_time(top_k_heap.insert, item, score, random.randint(1, 1000))
            heap_insert_times.append(exec_time)
        
        heap_extract_times = []
        for _ in range(100):
            if top_k_heap.size() > 0:
                _, exec_time = self.measure_time(top_k_heap.getTopK, 1)
                heap_extract_times.append(exec_time)
        
        heap_stats = {
            'avg_insert_time': statistics.mean(heap_insert_times),
            'avg_extract_time': statistics.mean(heap_extract_times) if heap_extract_times else 0,
            'total_operations': len(heap_insert_times) + len(heap_extract_times)
        }
        
        # æµ‹è¯•è‡ªå®šä¹‰é›†åˆæ€§èƒ½
        print("  æµ‹è¯•è‡ªå®šä¹‰é›†åˆæ€§èƒ½...")
        my_set = MySet()
        
        set_add_times = []
        for i in range(500):
            _, exec_time = self.measure_time(my_set.add, i)
            set_add_times.append(exec_time)
        
        set_contains_times = []
        for i in range(100):
            value = random.randint(0, 499)
            _, exec_time = self.measure_time(my_set.contains, value)
            set_contains_times.append(exec_time)
        
        set_stats = {
            'avg_add_time': statistics.mean(set_add_times),
            'avg_contains_time': statistics.mean(set_contains_times),
            'total_operations': len(set_add_times) + len(set_contains_times)
        }
        self.performance_data['data_structures'] = {
            'hashtable': hashtable_stats,
            'heap': heap_stats,
            'set': set_stats
        }
        
        print(f"âœ… æ•°æ®ç»“æ„æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print(f"   å“ˆå¸Œè¡¨å¹³å‡æ’å…¥æ—¶é—´: {hashtable_stats['avg_insert_time']:.6f}ms")
        print(f"   å †å¹³å‡æ’å…¥æ—¶é—´: {heap_stats['avg_insert_time']:.6f}ms")
        print(f"   é›†åˆå¹³å‡æ·»åŠ æ—¶é—´: {set_stats['avg_add_time']:.6f}ms")
    
    def test_concurrent_search_performance(self):
        """æµ‹è¯•å¹¶å‘æœç´¢æ€§èƒ½"""
        print("\nğŸ”„ æµ‹è¯•å¹¶å‘æœç´¢æ€§èƒ½...")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_keywords = ["å…¬å›­", "åšç‰©é¦†", "å±±", "æ¹–", "å¤"]
        test_types = self.all_types[:5]
        
        concurrent_results = {}
        
        for level in self.concurrent_levels:
            print(f"  æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            
            def search_task():
                """å•ä¸ªæœç´¢ä»»åŠ¡"""
                keyword = random.choice(test_keywords)
                start_time = time.time()
                self.spot_manager.getSpotByName(keyword)
                return (time.time() - start_time) * 1000
            
            # æ‰§è¡Œå¹¶å‘æµ‹è¯•
            with concurrent.futures.ThreadPoolExecutor(max_workers=level) as executor:
                start_time = time.time()
                futures = [executor.submit(search_task) for _ in range(level * 5)]
                search_times = [future.result() for future in concurrent.futures.as_completed(futures)]
                total_time = (time.time() - start_time) * 1000
            
            concurrent_results[level] = {
                'total_time': total_time,
                'avg_search_time': statistics.mean(search_times),
                'max_search_time': max(search_times),
                'min_search_time': min(search_times),
                'throughput': len(search_times) / (total_time / 1000)  # æ¯ç§’å¤„ç†æ•°
            }
            
            print(f"    æ€»è€—æ—¶: {total_time:.3f}ms")
            print(f"    å¹³å‡æœç´¢æ—¶é—´: {concurrent_results[level]['avg_search_time']:.3f}ms")
            print(f"    ååé‡: {concurrent_results[level]['throughput']:.2f} è¯·æ±‚/ç§’")
        
        self.performance_data['concurrent_performance'] = concurrent_results
        self.search_statistics['concurrent'] = concurrent_results
        
        print(f"âœ… å¹¶å‘æœç´¢æµ‹è¯•å®Œæˆ")
    
    def test_complex_search_scenarios(self):
        """æµ‹è¯•å¤æ‚æœç´¢åœºæ™¯"""
        print("\nğŸ¯ æµ‹è¯•å¤æ‚æœç´¢åœºæ™¯...")
        
        scenarios = []
        
        # åœºæ™¯1ï¼šå¤šå…³é”®å­—ç»„åˆæœç´¢
        print("  åœºæ™¯1: å¤šå…³é”®å­—ç»„åˆæœç´¢")
        multi_keyword_times = []
        keywords_combinations = [
            ["å…¬å›­", "åŒ—äº¬"], ["åšç‰©é¦†", "å†å²"], ["å±±", "ç™»é«˜"],
            ["æ¹–", "è‡ªç„¶"], ["å¤", "å»ºç­‘"]
        ]
        
        for keywords in keywords_combinations:
            times = []
            for keyword in keywords:
                result, exec_time = self.measure_time(
                    self.spot_manager.getSpotByName, keyword
                )
                times.append(exec_time)
            
            total_time = sum(times)
            multi_keyword_times.append(total_time)
            print(f"    å…³é”®å­—ç»„åˆ {keywords}: {total_time:.3f}ms")
        
        scenarios.append({
            'name': 'å¤šå…³é”®å­—ç»„åˆæœç´¢',
            'avg_time': statistics.mean(multi_keyword_times),
            'max_time': max(multi_keyword_times),
            'min_time': min(multi_keyword_times)
        })
        
        # åœºæ™¯2ï¼šç±»å‹ç­›é€‰ + æ’åº
        print("  åœºæ™¯2: ç±»å‹ç­›é€‰ + æ’åº")
        type_sort_times = []
        
        for spot_type in self.all_types[:5]:
            # å…ˆæŒ‰ç±»å‹ç­›é€‰
            filter_result, filter_time = self.measure_time(
                self.spot_manager.getTopKByType, spot_type, 100
            )
              # å†æ’åº
            sort_result, sort_time = self.measure_time(
                self.spot_manager.getAllSpotsSorted
            )
            
            total_time = filter_time + sort_time
            type_sort_times.append(total_time)
            print(f"    ç±»å‹ '{spot_type}' + æ’åº: {total_time:.3f}ms")
        
        scenarios.append({
            'name': 'ç±»å‹ç­›é€‰ + æ’åº',
            'avg_time': statistics.mean(type_sort_times),
            'max_time': max(type_sort_times),
            'min_time': min(type_sort_times)
        })
        
        # åœºæ™¯3ï¼šå¤§æ•°æ®é‡å¤„ç†
        print("  åœºæ™¯3: å¤§æ•°æ®é‡å¤„ç†")
        large_data_times = []
        
        for k in [100, 200, 500, 1000]:
            result, exec_time = self.measure_time(
                self.spot_manager.getTopKByType, self.all_types[0], k
            )
            large_data_times.append(exec_time)
            print(f"    è·å–Top {k}: {exec_time:.3f}ms")
        
        scenarios.append({
            'name': 'å¤§æ•°æ®é‡å¤„ç†',
            'avg_time': statistics.mean(large_data_times),
            'max_time': max(large_data_times),
            'min_time': min(large_data_times)
        })
        
        self.performance_data['complex_scenarios'] = scenarios
        
        print(f"âœ… å¤æ‚æœç´¢åœºæ™¯æµ‹è¯•å®Œæˆ")
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š...")
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = os.path.join(project_root, "performance_reports")
        os.makedirs(report_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_file = os.path.join(report_dir, f"spot_search_performance_report_{timestamp}.txt")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("æ™¯åŒºæœç´¢ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æµ‹è¯•æ™¯åŒºæ€»æ•°: {self.total_spots}\n")
            f.write(f"æµ‹è¯•è½®æ¬¡: {self.test_rounds}\n\n")
            
            # å…³é”®å­—æœç´¢æ€§èƒ½
            if 'keyword_search' in self.search_statistics:
                stats = self.search_statistics['keyword_search']
                f.write("1. å…³é”®å­—æœç´¢æ€§èƒ½\n")
                f.write("-" * 40 + "\n")
                f.write(f"å¹³å‡æœç´¢æ—¶é—´: {stats['avg_time']:.3f}ms\n")
                f.write(f"æœ€çŸ­æœç´¢æ—¶é—´: {stats['min_time']:.3f}ms\n")
                f.write(f"æœ€é•¿æœç´¢æ—¶é—´: {stats['max_time']:.3f}ms\n")
                f.write(f"æ—¶é—´æ ‡å‡†å·®: {stats['std_time']:.3f}ms\n")
                f.write(f"å¹³å‡ç»“æœæ•°é‡: {stats['avg_results']:.1f}\n")
                f.write(f"æµ‹è¯•å…³é”®å­—æ•°: {stats['total_keywords_tested']}\n\n")
            
            # ç±»å‹æœç´¢æ€§èƒ½
            if 'type_search' in self.search_statistics:
                stats = self.search_statistics['type_search']
                f.write("2. ç±»å‹æœç´¢æ€§èƒ½\n")
                f.write("-" * 40 + "\n")
                f.write(f"å¹³å‡æœç´¢æ—¶é—´: {stats['avg_time']:.3f}ms\n")
                f.write(f"æœ€çŸ­æœç´¢æ—¶é—´: {stats['min_time']:.3f}ms\n")
                f.write(f"æœ€é•¿æœç´¢æ—¶é—´: {stats['max_time']:.3f}ms\n")
                f.write(f"æ—¶é—´æ ‡å‡†å·®: {stats['std_time']:.3f}ms\n")
                f.write(f"æµ‹è¯•ç±»å‹æ•°: {stats['total_types_tested']}\n\n")
            
            # æ’åºæ€§èƒ½
            if 'sorting' in self.search_statistics:
                f.write("3. æ’åºæ€§èƒ½\n")
                f.write("-" * 40 + "\n")
                for method, stats in self.search_statistics['sorting'].items():
                    f.write(f"{method}æ’åº:\n")
                    f.write(f"  å¹³å‡æ—¶é—´: {stats['avg_time']:.3f}ms\n")
                    f.write(f"  æœ€çŸ­æ—¶é—´: {stats['min_time']:.3f}ms\n")
                    f.write(f"  æœ€é•¿æ—¶é—´: {stats['max_time']:.3f}ms\n")
                f.write("\n")
            
            # æ•°æ®ç»“æ„æ€§èƒ½
            if 'data_structures' in self.performance_data:
                f.write("4. æ•°æ®ç»“æ„æ€§èƒ½\n")
                f.write("-" * 40 + "\n")
                ds_stats = self.performance_data['data_structures']
                
                f.write(f"å“ˆå¸Œè¡¨:\n")
                f.write(f"  å¹³å‡æ’å…¥æ—¶é—´: {ds_stats['hashtable']['avg_insert_time']:.6f}ms\n")
                f.write(f"  å¹³å‡æŸ¥æ‰¾æ—¶é—´: {ds_stats['hashtable']['avg_search_time']:.6f}ms\n")
                
                f.write(f"å †:\n")
                f.write(f"  å¹³å‡æ’å…¥æ—¶é—´: {ds_stats['heap']['avg_insert_time']:.6f}ms\n")
                f.write(f"  å¹³å‡æå–æ—¶é—´: {ds_stats['heap']['avg_extract_time']:.6f}ms\n")
                
                f.write(f"è‡ªå®šä¹‰é›†åˆ:\n")
                f.write(f"  å¹³å‡æ·»åŠ æ—¶é—´: {ds_stats['set']['avg_add_time']:.6f}ms\n")
                f.write(f"  å¹³å‡æŸ¥æ‰¾æ—¶é—´: {ds_stats['set']['avg_contains_time']:.6f}ms\n\n")
            
            # å¹¶å‘æ€§èƒ½
            if 'concurrent' in self.search_statistics:
                f.write("5. å¹¶å‘æœç´¢æ€§èƒ½\n")
                f.write("-" * 40 + "\n")
                for level, stats in self.search_statistics['concurrent'].items():
                    f.write(f"å¹¶å‘çº§åˆ« {level}:\n")
                    f.write(f"  å¹³å‡æœç´¢æ—¶é—´: {stats['avg_search_time']:.3f}ms\n")
                    f.write(f"  ååé‡: {stats['throughput']:.2f} è¯·æ±‚/ç§’\n")
                f.write("\n")
            
            # å¤æ‚åœºæ™¯
            if 'complex_scenarios' in self.performance_data:
                f.write("6. å¤æ‚æœç´¢åœºæ™¯\n")
                f.write("-" * 40 + "\n")
                for scenario in self.performance_data['complex_scenarios']:
                    f.write(f"{scenario['name']}:\n")
                    f.write(f"  å¹³å‡æ—¶é—´: {scenario['avg_time']:.3f}ms\n")
                    f.write(f"  æœ€çŸ­æ—¶é—´: {scenario['min_time']:.3f}ms\n")
                    f.write(f"  æœ€é•¿æ—¶é—´: {scenario['max_time']:.3f}ms\n")
        
        print(f"âœ… æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        # ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_file = os.path.join(report_dir, f"spot_search_performance_data_{timestamp}.json")
        
        report_data = {
            'test_info': {
                'timestamp': timestamp,
                'total_spots': self.total_spots,
                'test_rounds': self.test_rounds,
                'spot_types': self.all_types
            },
            'performance_statistics': self.search_statistics,
            'raw_performance_data': dict(self.performance_data)
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜: {json_file}")
        
        return report_file, json_file
    
    def create_performance_visualizations(self):
        """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“ˆ ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # é‡æ–°è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå› ä¸ºstyle.useä¼šé‡ç½®é…ç½®ï¼‰
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨ç›®å½•
        charts_dir = os.path.join(project_root, "performance_charts")
        os.makedirs(charts_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")        # 1. ä¿®æ”¹æœç´¢æ—¶é—´åˆ†å¸ƒå›¾
        if 'keyword_search_times_by_length' in self.performance_data and 'type_search_times' in self.performance_data:
            # ç¡®ä¿ä¸­æ–‡å­—ä½“è®¾ç½®
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
            plt.rcParams['font.family'] = 'sans-serif'
            plt.rcParams['axes.unicode_minus'] = False
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # å·¦å›¾ï¼šä¸åŒå…³é”®å­—é•¿åº¦çš„æœç´¢æ—¶é—´åˆ†å¸ƒï¼ˆæŠ˜çº¿å›¾ï¼Œæ¨ªåæ ‡ä¸ºå…³é”®å­—é•¿åº¦ï¼Œçºµåæ ‡ä¸ºæœç´¢æ—¶é—´ï¼‰
            lengths = list(range(1, 9))  # 1åˆ°8çš„å…³é”®å­—é•¿åº¦
            avg_times = []
            
            for length in lengths:
                if length in self.performance_data['keyword_search_times_by_length'] and self.performance_data['keyword_search_times_by_length'][length]:
                    avg_time = statistics.mean(self.performance_data['keyword_search_times_by_length'][length])
                    avg_times.append(avg_time)
                else:
                    avg_times.append(0)  # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè®¾ä¸º0
            
            # ç»˜åˆ¶æŠ˜çº¿å›¾
            ax1.plot(lengths, avg_times, 'o-', linewidth=2, markersize=8, color='skyblue', markerfacecolor='blue')
            ax1.set_title('ä¸åŒå…³é”®å­—é•¿åº¦æœç´¢æ—¶é—´åˆ†å¸ƒ')
            ax1.set_xlabel('å…³é”®å­—é•¿åº¦')
            ax1.set_ylabel('æœç´¢æ—¶é—´ (ms)')
            ax1.grid(True, alpha=0.3)
            ax1.set_xticks(lengths)  # è®¾ç½®xè½´åˆ»åº¦ä¸º1-8
            
            # åœ¨æ¯ä¸ªç‚¹ä¸Šæ ‡æ³¨æ•°å€¼
            for length, avg_time in zip(lengths, avg_times):
                if avg_time > 0:
                    ax1.annotate(f'{avg_time:.3f}', (length, avg_time), 
                               textcoords="offset points", xytext=(0,10), ha='center')
              # å³å›¾ï¼šç±»å‹æœç´¢æ—¶é—´åˆ†å¸ƒï¼ˆæŠ˜çº¿å›¾ï¼Œæ¨ªçºµåæ ‡äº¤æ¢ï¼šæ¨ªåæ ‡ä¸ºé¢‘æ¬¡ï¼Œçºµåæ ‡ä¸ºæœç´¢æ—¶é—´ï¼‰
            # è®¡ç®—æœç´¢æ—¶é—´çš„åˆ†å¸ƒ
            type_times = self.performance_data['type_search_times']
            if type_times:
                # å°†æœç´¢æ—¶é—´æŒ‰åŒºé—´åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªåŒºé—´çš„é¢‘æ¬¡
                min_time = min(type_times)
                max_time = max(type_times)
                time_range = max_time - min_time
                
                # åˆ›å»º20ä¸ªåŒºé—´
                frequencies = list(range(0, 21))  # é¢‘æ¬¡ä»0åˆ°20
                time_values = []
                
                # ä¸ºæ¯ä¸ªé¢‘æ¬¡è®¡ç®—å¯¹åº”çš„æœç´¢æ—¶é—´å€¼
                for freq in frequencies:
                    # çº¿æ€§æ’å€¼è®¡ç®—å¯¹åº”çš„æœç´¢æ—¶é—´
                    if freq == 0:
                        time_val = min_time
                    elif freq == 20:
                        time_val = max_time
                    else:
                        # æ ¹æ®é¢‘æ¬¡åœ¨0-20èŒƒå›´å†…çš„ä½ç½®ï¼Œæ’å€¼è®¡ç®—å¯¹åº”çš„æ—¶é—´
                        progress = freq / 20.0
                        time_val = min_time + progress * time_range
                    time_values.append(time_val)
                
                # ç»˜åˆ¶æŠ˜çº¿å›¾
                ax2.plot(frequencies, time_values, 'o-', linewidth=2, markersize=6, color='lightcoral', markerfacecolor='red')
                ax2.set_title('ç±»å‹æœç´¢æ—¶é—´åˆ†å¸ƒ')
                ax2.set_xlabel('é¢‘æ¬¡')
                ax2.set_ylabel('æœç´¢æ—¶é—´ (ms)')
                ax2.grid(True, alpha=0.3)
                ax2.set_xticks(range(0, 21, 2))  # è®¾ç½®xè½´åˆ»åº¦ä¸º0,2,4,...,20
            else:
                # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œç»˜åˆ¶ç©ºå›¾
                ax2.text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=ax2.transAxes)
                ax2.set_title('ç±»å‹æœç´¢æ—¶é—´åˆ†å¸ƒ')
                ax2.set_xlabel('é¢‘æ¬¡')
                ax2.set_ylabel('æœç´¢æ—¶é—´ (ms)')
            
            plt.tight_layout()
            chart_file = os.path.join(charts_dir, f"search_time_distribution_{timestamp}.png")
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"  æœç´¢æ—¶é—´åˆ†å¸ƒå›¾: {chart_file}")        
        # 2. å¹¶å‘æ€§èƒ½å›¾
        if 'concurrent_performance' in self.performance_data:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            levels = list(self.performance_data['concurrent_performance'].keys())
            avg_times = [self.performance_data['concurrent_performance'][level]['avg_search_time'] 
                        for level in levels]
            throughputs = [self.performance_data['concurrent_performance'][level]['throughput'] 
                          for level in levels]
            
            # å¹³å‡å“åº”æ—¶é—´ vs å¹¶å‘çº§åˆ«
            ax1.plot(levels, avg_times, 'o-', linewidth=2, markersize=8, color='red')
            ax1.set_title('å¹¶å‘çº§åˆ« vs å¹³å‡å“åº”æ—¶é—´')
            ax1.set_xlabel('å¹¶å‘çº§åˆ«')
            ax1.set_ylabel('å¹³å‡å“åº”æ—¶é—´ (ms)')
            ax1.grid(True, alpha=0.3)
            
            # ååé‡ vs å¹¶å‘çº§åˆ«
            ax2.plot(levels, throughputs, 's-', linewidth=2, markersize=8, color='green')
            ax2.set_title('å¹¶å‘çº§åˆ« vs ååé‡')
            ax2.set_xlabel('å¹¶å‘çº§åˆ«')
            ax2.set_ylabel('ååé‡ (è¯·æ±‚/ç§’)')
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            chart_file = os.path.join(charts_dir, f"concurrent_performance_{timestamp}.png")
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"  å¹¶å‘æ€§èƒ½å›¾: {chart_file}")        
        print(f"âœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆåˆ°ç›®å½•: {charts_dir}")
    
    def run_complete_performance_test(self):
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ™¯åŒºæœç´¢ç³»ç»Ÿå®Œæ•´æ€§èƒ½æµ‹è¯•\n")
        
        start_time = time.time()
        
        try:
            # 1. å…³é”®å­—æœç´¢æ€§èƒ½æµ‹è¯•
            self.test_keyword_search_performance()
            
            # 2. ç±»å‹æœç´¢æ€§èƒ½æµ‹è¯•
            self.test_type_based_search_performance()
            
            # 3. æ’åºæ€§èƒ½æµ‹è¯•
            self.test_sorting_performance()
            
            # 4. æ•°æ®ç»“æ„æ€§èƒ½æµ‹è¯•
            self.test_data_structure_performance()
            
            # 5. å¹¶å‘æœç´¢æ€§èƒ½æµ‹è¯•
            self.test_concurrent_search_performance()
            
            # 6. å¤æ‚æœç´¢åœºæ™¯æµ‹è¯•
            self.test_complex_search_scenarios()
            
            # 7. ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨
            report_file, json_file = self.generate_performance_report()
            self.create_performance_visualizations()
            
            total_time = time.time() - start_time
            
            print(f"\nğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆ!")
            print(f"â±ï¸ æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
            print(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Š: {report_file}")
            print(f"ğŸ“Š è¯¦ç»†æ•°æ®: {json_file}")
            
            # è¾“å‡ºå…³é”®æ€§èƒ½æŒ‡æ ‡æ‘˜è¦
            print(f"\nğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡æ‘˜è¦:")
            if 'keyword_search' in self.search_statistics:
                print(f"  å…³é”®å­—æœç´¢å¹³å‡æ—¶é—´: {self.search_statistics['keyword_search']['avg_time']:.3f}ms")
            if 'type_search' in self.search_statistics:
                print(f"  ç±»å‹æœç´¢å¹³å‡æ—¶é—´: {self.search_statistics['type_search']['avg_time']:.3f}ms")
            if 'concurrent' in self.search_statistics and 10 in self.search_statistics['concurrent']:
                print(f"  10å¹¶å‘å¹³å‡å“åº”æ—¶é—´: {self.search_statistics['concurrent'][10]['avg_search_time']:.3f}ms")
                print(f"  10å¹¶å‘ååé‡: {self.search_statistics['concurrent'][10]['throughput']:.2f} è¯·æ±‚/ç§’")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    setup_chinese_font()
    
    print("=" * 80)
    print("æ™¯åŒºæœç´¢ç³»ç»Ÿæ€§èƒ½æµ‹è¯•ç¨‹åº")
    print("=" * 80)
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œæµ‹è¯•
        tester = SpotSearchPerformanceTester()
        tester.run_complete_performance_test()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
